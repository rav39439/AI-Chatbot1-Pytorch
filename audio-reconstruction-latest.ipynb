{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# #-------------------------------------------------modified..................\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n# ------------------------\n# Encoder / Decoder\n# -------------------------\nclass Encoder(nn.Module):\n    def __init__(self, in_channels=1, z_dim=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(256, z_dim, kernel_size=4, stride=2, padding=1)\n        )\n\n    def forward(self, x):\n        return self.net(x)  # (B, D, T_down)\n\nclass Decoder(nn.Module):\n    def __init__(self, out_channels=1, z_dim=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ConvTranspose1d(z_dim, 256, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose1d(64, out_channels, kernel_size=4, stride=2, padding=1)\n        )\n\n    def forward(self, q):\n        return self.net(q)\n\n# -------------------------\n# Vector Quantizer EMA\n# -------------------------\n# class VectorQuantizerEMA(nn.Module):\n#     def __init__(self, num_embeddings=1024, embedding_dim=256, commitment_cost=0.25, decay=0.99, eps=1e-5):\n#         super().__init__()\n#         self.num_embeddings = num_embeddings\n#         self.embedding_dim = embedding_dim\n#         self.commitment_cost = commitment_cost\n#         self.decay = decay\n#         self.eps = eps\n\n#         embed = torch.randn(embedding_dim, num_embeddings)\n#         self.register_buffer('embedding', embed)  # (D, K)\n#         self.register_buffer('cluster_size', torch.zeros(num_embeddings))\n#         self.register_buffer('embed_avg', embed.clone())\n\n#     def forward(self, z):\n#         B, D, T = z.shape\n#         flat = z.permute(0,2,1).contiguous().view(-1, D)  # (B*T, D)\n#         emb_t = self.embedding.t()  # (K, D)\n\n#         # Compute distances and nearest embeddings\n#         distances = flat.pow(2).sum(1, keepdim=True) - 2 * flat @ emb_t.t() + emb_t.pow(2).sum(1).unsqueeze(0)\n#         encoding_indices = torch.argmin(distances, dim=1)\n#         encodings = F.one_hot(encoding_indices, num_classes=self.num_embeddings).type(flat.dtype)\n#         quantized = (encodings @ self.embedding.t()).view(B, T, D).permute(0,2,1).contiguous()\n\n#         # EMA updates (training only)\n#         if self.training:\n#             with torch.no_grad():  # do EMA updates without tracking autograd\n#                 n = encodings.sum(0).detach()\n#                 self.cluster_size.mul_(self.decay).add_(n, alpha=1 - self.decay)\n#                 embed_sum = flat.t() @ encodings\n#                 self.embed_avg.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n#                 n = self.cluster_size + self.eps\n#                 self.embedding.copy_(self.embed_avg / n.unsqueeze(0))\n#         # Compute VQ losses\n#         e_latent_loss = F.mse_loss(quantized.detach(), z)\n#         q_latent_loss = F.mse_loss(quantized, z.detach())\n#         loss = q_latent_loss + self.commitment_cost * e_latent_loss\n\n#         # Straight-through estimator\n#         quantized = z + (quantized - z).detach()\n#         indices = encoding_indices.view(B, T)\n#         return quantized, loss, indices\n\nclass VectorQuantizerEMA(nn.Module):\n    def __init__(self, num_embeddings=1024, embedding_dim=256,\n                 commitment_cost=0.25, decay=0.99, eps=1e-5):\n        super().__init__()\n\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.commitment_cost = commitment_cost\n        self.decay = decay\n        self.eps = eps\n\n        # Much better initialization\n        embed = torch.randn(num_embeddings, embedding_dim) * 0.1\n        self.register_buffer(\"embedding\", embed)            # (K, D)\n\n        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n        self.register_buffer(\"embed_avg\", embed.clone())\n\n    def forward(self, z):\n        \"\"\"\n        z: (B, D, T)\n        \"\"\"\n        B, D, T = z.shape\n        flat = z.permute(0, 2, 1).contiguous().view(-1, D)  # (B*T, D)\n\n        # ---------- Correct pairwise L2 distances ----------\n        emb = self.embedding                           # (K, D)\n        dist = (flat.pow(2).sum(dim=1, keepdim=True)\n                + emb.pow(2).sum(dim=1)\n                - 2 * flat @ emb.t())                  # (B*T, K)\n\n        # ---------- Find nearest embedding ----------\n        encoding_indices = torch.argmin(dist, dim=1)\n        encodings = F.one_hot(encoding_indices,\n                               self.num_embeddings).type(flat.dtype)\n\n        quantized = encodings @ emb        # (B*T, D)\n        quantized = quantized.view(B, T, D).permute(0, 2, 1).contiguous()\n\n        # ---------- EMA update ----------\n        if self.training:\n            with torch.no_grad():\n\n                # cluster size update with smoothing\n                cluster_sum = encodings.sum(0)\n                self.cluster_size.mul_(self.decay).add_(\n                    cluster_sum, alpha=1 - self.decay)\n\n                # embedding sum\n                embed_sum = flat.t() @ encodings\n                self.embed_avg.mul_(self.decay).add_(\n                    embed_sum.t(), alpha=1 - self.decay)\n\n                # Laplace smoothing to prevent collapse\n                n = (self.cluster_size + self.eps)\n                embed_normalized = self.embed_avg / n.unsqueeze(1)\n\n                self.embedding.copy_(embed_normalized)\n\n        # ---------- Losses ----------\n        e_loss = F.mse_loss(z.detach(), quantized)\n        q_loss = F.mse_loss(z, quantized.detach())\n\n        vq_loss = q_loss + self.commitment_cost * e_loss\n\n        # Straight-through estimator\n        quantized = z + (quantized - z).detach()\n\n        indices = encoding_indices.view(B, T)\n        return quantized, vq_loss, indices\n\n# ------------------------\n# VQ-VAE wrapper\n# -------------------------\nclass VQVAE(nn.Module):\n    def __init__(self, z_dim=256, num_embeddings=1024):\n        super().__init__()\n        self.encoder = Encoder(1, z_dim)\n        self.quantizer = VectorQuantizerEMA(num_embeddings, z_dim)\n        self.decoder = Decoder(1, z_dim)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        quantized, vq_loss, indices = self.quantizer(z)\n        x_rec = self.decoder(quantized)\n        return x_rec, vq_loss, indices\n\n# ------------------------\n# Explicit training step\n# -------------------------\n# def training_step(model, optimizer, batch_waveform):\n#     model.train()\n#     optimizer.zero_grad()\n#     x_rec, vq_loss, _ = model(batch_waveform)\n#     recon_loss = F.l1_loss(x_rec, batch_waveform)\n#     loss = recon_loss + vq_loss\n#     loss.backward()\n#     optimizer.step()\n#     return loss.item(), recon_loss.item(), vq_loss.item()\n\n# ------------------------\n# Eval helpers (no gradient)\n# -------------------------\n@torch.no_grad()\ndef tokenize_audio(model, waveform):\n    model.eval()\n    z = model.encoder(waveform)\n    _, _, indices = model.quantizer(z)\n    return indices.cpu().numpy()\n\n@torch.no_grad()\ndef reconstruct_from_indices(model, indices):\n    if isinstance(indices, np.ndarray):\n        indices = torch.from_numpy(indices).to(model.quantizer.embedding.device)\n    B, T = indices.shape\n    D = model.quantizer.embedding.shape[0]\n    emb = model.quantizer.embedding.t()\n    flat = emb[indices.view(-1)]\n    q = flat.view(B, T, D).permute(0,2,1).contiguous()\n    x_rec = model.decoder(q)\n    return x_rec\n\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# ------------------------\n# Assume your VQVAE, Encoder, Decoder, VectorQuantizerEMA are already defined\n# ------------------------\n\n# Create dummy dataset: 10 audio samples, each length 1024\nB = 10      # batch size\nT = 1024    # audio length\ndummy_audio = torch.randn(B, 1, T)  # (B,1,T)\n\n# Wrap in a DataLoader\ndataset = TensorDataset(dummy_audio)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n\n# Create model\nz_dim = 64           # smaller for testing\nnum_embeddings = 32  # smaller for testing\nmodel = VQVAE(z_dim=z_dim, num_embeddings=num_embeddings)\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# ------------------------\n# Single training step on dummy data\n# ------------------------\nfor batch in dataloader:\n    x = batch[0].to(device)  # (B,1,T)\n    \n    model.train()\n    optimizer.zero_grad()\n    \n    x_rec, vq_loss, indices = model(x)\n    \n    # Reconstruction loss (L1)\n    recon_loss = F.l1_loss(x_rec, x)\n    \n    # Total loss\n    loss = recon_loss + vq_loss\n    print(loss)\n    # Backprop\n    loss.backward()\n    optimizer.step()\n    \n    # print(\"Batch Loss:\", loss.item())\n    # print(\"Reconstruction Loss:\", recon_loss.item())\n    # print(\"VQ Loss:\", vq_loss.item())\n    # print(\"Token shape:\", indices.shape)\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:25:31.641565Z","iopub.execute_input":"2025-12-08T11:25:31.641810Z","iopub.status.idle":"2025-12-08T11:25:41.771043Z","shell.execute_reply.started":"2025-12-08T11:25:31.641787Z","shell.execute_reply":"2025-12-08T11:25:41.770150Z"}},"outputs":[{"name":"stdout","text":"tensor(0.8597, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.8286, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.8114, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.8041, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.8006, device='cuda:0', grad_fn=<AddBackward0>)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class AudioDataset(torch.utils.data.Dataset):\n    def __init__(self, files, segment_length=16000):\n        self.files = files\n        self.segment_length = segment_length\n        self.sr = 16000\n\n    def __getitem__(self, idx):\n        wav, sr = torchaudio.load(self.files[idx])\n        wav = torchaudio.functional.resample(wav, sr, self.sr)\n        wav = wav.mean(dim=0)   # mono\n        wav = wav / wav.abs().max()  # normalize\n        \n        # random crop\n        if wav.shape[0] >= self.segment_length:\n            start = torch.randint(0, wav.shape[0] - self.segment_length, (1,))\n            wav = wav[start:start+self.segment_length]\n        else:\n            wav = F.pad(wav, (0, self.segment_length - wav.shape[0]))\n        \n        return wav.unsqueeze(0)\n\n    def __len__(self):\n        return len(self.files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:25:45.748753Z","iopub.execute_input":"2025-12-08T11:25:45.749135Z","iopub.status.idle":"2025-12-08T11:25:45.755195Z","shell.execute_reply.started":"2025-12-08T11:25:45.749113Z","shell.execute_reply":"2025-12-08T11:25:45.754488Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nimport os\nimport torch\nimport torchaudio\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nAUDIO_DIR = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/blues\"\n\n# collect audio file paths\naudio_files = [\n    os.path.join(AUDIO_DIR, f)\n    for f in os.listdir(AUDIO_DIR)\n    if f.lower().endswith((\".wav\", \".mp3\", \".flac\"))\n]\nprint(\"Found\", len(audio_files), \"audio files\")\n\naudio_files = []\nfor root, dirs, files in os.walk(AUDIO_DIR):\n    for f in files:\n        if f.lower().endswith((\".wav\", \".mp3\", \".flac\")):\n            audio_files.append(os.path.join(root, f))\n\ndataset = AudioDataset(audio_files, segment_length=16000)\n\nloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:25:50.399819Z","iopub.execute_input":"2025-12-08T11:25:50.400125Z","iopub.status.idle":"2025-12-08T11:25:51.796507Z","shell.execute_reply.started":"2025-12-08T11:25:50.400076Z","shell.execute_reply":"2025-12-08T11:25:51.795837Z"}},"outputs":[{"name":"stdout","text":"Found 100 audio files\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = VQVAE(z_dim=256, num_embeddings=1024).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:25:58.847562Z","iopub.execute_input":"2025-12-08T11:25:58.847867Z","iopub.status.idle":"2025-12-08T11:25:58.876753Z","shell.execute_reply.started":"2025-12-08T11:25:58.847842Z","shell.execute_reply":"2025-12-08T11:25:58.875904Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nimport torch\nimport torch.nn.functional as F\n\ndef multiscale_stft_loss(x, x_rec):\n    \"\"\"\n    x, x_rec: (B, 1, T)\n    Returns scalar STFT loss\n    \"\"\"\n    losses = []\n    scales = [\n        (2048, 512, 2048),\n        (1024, 256, 1024),\n        (512, 128, 512),\n    ]\n\n    x = x[:, 0]       # (B, T)\n    x_rec = x_rec[:, 0]\n\n    for n_fft, hop, win in scales:\n        window = torch.hann_window(win).to(x.device).detach()\n\n        X = torch.stft(\n            x,\n            n_fft=n_fft,\n            hop_length=hop,\n            win_length=win,\n            window=window,\n            return_complex=True\n        )\n\n        Xr = torch.stft(\n            x_rec,\n            n_fft=n_fft,\n            hop_length=hop,\n            win_length=win,\n            window=window,\n            return_complex=True\n        )\n\n        losses.append((X - Xr).abs().mean())\n\n    return sum(losses)\n\n# import torch\n# import torch.nn.functional as F\n\n# class multiscale_stft_loss(torch.nn.Module):\n#     def __init__(self, scales=None):\n#         super().__init__()\n\n#         if scales is None:\n#             scales = [\n#                 (2048, 512, 2048),\n#                 (1024, 256, 1024),\n#                 (512, 128, 512),\n#             ]\n\n#         self.scales = scales\n#         self.windows = {\n#             win: torch.hann_window(win) for (_, _, win) in scales\n#         }\n\n#     def stft(self, x, n_fft, hop, win, device):\n#         window = self.windows[win].to(device)\n#         return torch.stft(\n#             x,\n#             n_fft=n_fft,\n#             hop_length=hop,\n#             win_length=win,\n#             window=window,\n#             return_complex=True,\n#         )\n\n#     def forward(self, x, x_rec):\n#         \"\"\"\n#         x, x_rec: (B,1,T)\n#         \"\"\"\n#         x = x[:, 0]\n#         x_rec = x_rec[:, 0]\n\n#         sc_losses = []\n#         mag_losses = []\n\n#         for n_fft, hop, win in self.scales:\n#             X = self.stft(x, n_fft, hop, win, x.device)\n#             Xr = self.stft(x_rec, n_fft, hop, win, x.device)\n\n#             # magnitude spectrograms\n#             mag = X.abs()\n#             mag_r = Xr.abs()\n\n#             # 1) Spectral Convergence (stabilizes training)\n#             sc = torch.norm(mag - mag_r, p=\"fro\") / (torch.norm(mag, p=\"fro\") + 1e-8)\n#             sc_losses.append(sc)\n\n#             # 2) Log-magnitude L1 loss (prevents large spikes)\n#             mag_l1 = F.l1_loss(torch.log1p(mag), torch.log1p(mag_r))\n#             mag_losses.append(mag_l1)\n\n#         # Weighted sum\n#         loss = (\n#             sum(sc_losses) / len(sc_losses) +\n#             sum(mag_losses) / len(mag_losses)\n#         )\n\n#         return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:27:58.994400Z","iopub.execute_input":"2025-12-08T11:27:58.995032Z","iopub.status.idle":"2025-12-08T11:27:59.001692Z","shell.execute_reply.started":"2025-12-08T11:27:58.995010Z","shell.execute_reply":"2025-12-08T11:27:59.000790Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# num_epochs = 50\n\n# for epoch in range(num_epochs):\n#     total_loss = 0.0\n#     total_recon = 0.0\n#     total_vq = 0.0\n\n#     for batch_idx, batch_waveform in enumerate(loader):\n\n#         # Move batch to device\n#         x = batch_waveform.to(device)    # <-- you used batch_waveform but later referenced x\n#         optimizer.zero_grad()\n#         model.train()\n\n#         # Forward pass\n#         x_rec, vq_loss, indices = model(x)\n\n#         # Reconstruction loss\n#         recon_loss = F.l1_loss(x_rec, x)\n\n#         # Total loss\n#         loss = recon_loss + vq_loss\n\n#         # Backprop\n#         loss.backward()\n#         optimizer.step()\n\n#         # Track for epoch summary\n#         total_loss += loss.item()\n#         total_recon += recon_loss.item()\n#         total_vq += vq_loss.item()\n\n#         # Print progress\n#         if batch_idx % 10 == 0:\n#             print(\n#                 f\"Epoch [{epoch+1}/{num_epochs}], \"\n#                 f\"Batch [{batch_idx}/{len(loader)}], \"\n#                 f\"Loss: {loss.item():.4f}, \"\n#                 f\"Recon: {recon_loss.item():.4f}, \"\n#                 f\"VQ: {vq_loss.item():.4f}\"\n#             )\n\n#     # End-of-epoch summary\n#     print(\n#         f\"\\nEpoch [{epoch+1}/{num_epochs}] SUMMARY:\\n\"\n#         f\"  Avg Loss: {total_loss / len(loader):.4f}\\n\"\n#         f\"  Avg Recon: {total_recon / len(loader):.4f}\\n\"\n#         f\"  Avg VQ: {total_vq / len(loader):.4f}\\n\"\n#     )\n\n\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n    total_l1 = 0.0\n    total_stft = 0.0\n    total_vq = 0.0\n\n    for batch_idx, batch_waveform in enumerate(loader):\n\n        x = batch_waveform.to(device)  # (B,1,T)\n        optimizer.zero_grad()\n        model.train()\n\n        # --------------------\n        # Forward pass\n        # --------------------\n        x_rec, vq_loss, indices = model(x)\n\n        # Waveform L1 loss\n        l1_loss = F.l1_loss(x_rec, x)\n\n        # Multi-scale STFT loss\n        stft_loss = multiscale_stft_loss(x, x_rec)\n\n        # Combined loss\n        loss = l1_loss + 0.1 * stft_loss + vq_loss\n\n        # --------------------\n        # Backprop\n        # --------------------\n        loss.backward()\n        optimizer.step()\n\n        # Track for epoch\n        total_loss += loss.item()\n        total_l1 += l1_loss.item()\n        total_stft += stft_loss.item()\n        total_vq += vq_loss.item()\n\n        if batch_idx % 10 == 0:\n            print(\n                f\"Epoch [{epoch+1}/{num_epochs}] \"\n                f\"Batch [{batch_idx}/{len(loader)}] | \"\n                f\"Loss: {loss.item():.4f} | \"\n                f\"L1: {l1_loss.item():.4f} | \"\n                f\"STFT: {stft_loss.item():.4f} | \"\n                f\"VQ: {vq_loss.item():.4f}\"\n            )\n\n    # End-of-epoch summary\n    print(\n        f\"\\nEPOCH {epoch+1} SUMMARY:\\n\"\n        f\"  Avg Loss : {total_loss/len(loader):.4f}\\n\"\n        f\"  Avg L1   : {total_l1/len(loader):.4f}\\n\"\n        f\"  Avg STFT : {total_stft/len(loader):.4f}\\n\"\n        f\"  Avg VQ   : {total_vq/len(loader):.4f}\\n\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:28:05.111200Z","iopub.execute_input":"2025-12-08T11:28:05.111801Z","iopub.status.idle":"2025-12-08T11:28:54.269087Z","shell.execute_reply.started":"2025-12-08T11:28:05.111778Z","shell.execute_reply":"2025-12-08T11:28:54.268404Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20] Batch [0/13] | Loss: 1.1143 | L1: 0.5181 | STFT: 5.9614 | VQ: 0.0000\nEpoch [1/20] Batch [10/13] | Loss: 0.9586 | L1: 0.3955 | STFT: 5.5985 | VQ: 0.0033\n\nEPOCH 1 SUMMARY:\n  Avg Loss : 1.0040\n  Avg L1   : 0.4449\n  Avg STFT : 5.5756\n  Avg VQ   : 0.0016\n\nEpoch [2/20] Batch [0/13] | Loss: 0.8642 | L1: 0.3400 | STFT: 5.1398 | VQ: 0.0102\nEpoch [2/20] Batch [10/13] | Loss: 0.7086 | L1: 0.2063 | STFT: 4.0496 | VQ: 0.0973\n\nEPOCH 2 SUMMARY:\n  Avg Loss : 0.7130\n  Avg L1   : 0.2209\n  Avg STFT : 4.3223\n  Avg VQ   : 0.0600\n\nEpoch [3/20] Batch [0/13] | Loss: 0.7198 | L1: 0.1994 | STFT: 4.8607 | VQ: 0.0343\nEpoch [3/20] Batch [10/13] | Loss: 0.6047 | L1: 0.1627 | STFT: 4.3629 | VQ: 0.0057\n\nEPOCH 3 SUMMARY:\n  Avg Loss : 0.5437\n  Avg L1   : 0.1528\n  Avg STFT : 3.8292\n  Avg VQ   : 0.0081\n\nEpoch [4/20] Batch [0/13] | Loss: 0.4791 | L1: 0.1229 | STFT: 3.4946 | VQ: 0.0067\nEpoch [4/20] Batch [10/13] | Loss: 0.3771 | L1: 0.1077 | STFT: 2.6318 | VQ: 0.0062\n\nEPOCH 4 SUMMARY:\n  Avg Loss : 0.4719\n  Avg L1   : 0.1272\n  Avg STFT : 3.3820\n  Avg VQ   : 0.0065\n\nEpoch [5/20] Batch [0/13] | Loss: 0.5238 | L1: 0.1371 | STFT: 3.7917 | VQ: 0.0076\nEpoch [5/20] Batch [10/13] | Loss: 0.3962 | L1: 0.1240 | STFT: 2.6190 | VQ: 0.0103\n\nEPOCH 5 SUMMARY:\n  Avg Loss : 0.4880\n  Avg L1   : 0.1319\n  Avg STFT : 3.4774\n  Avg VQ   : 0.0083\n\nEpoch [6/20] Batch [0/13] | Loss: 0.6757 | L1: 0.1608 | STFT: 5.0214 | VQ: 0.0127\nEpoch [6/20] Batch [10/13] | Loss: 0.6145 | L1: 0.1578 | STFT: 4.1184 | VQ: 0.0448\n\nEPOCH 6 SUMMARY:\n  Avg Loss : 0.4983\n  Avg L1   : 0.1285\n  Avg STFT : 3.4021\n  Avg VQ   : 0.0296\n\nEpoch [7/20] Batch [0/13] | Loss: 0.5134 | L1: 0.1183 | STFT: 3.2756 | VQ: 0.0675\nEpoch [7/20] Batch [10/13] | Loss: 0.5808 | L1: 0.1155 | STFT: 2.7744 | VQ: 0.1879\n\nEPOCH 7 SUMMARY:\n  Avg Loss : 0.5745\n  Avg L1   : 0.1235\n  Avg STFT : 3.2120\n  Avg VQ   : 0.1297\n\nEpoch [8/20] Batch [0/13] | Loss: 0.5867 | L1: 0.1201 | STFT: 2.6833 | VQ: 0.1983\nEpoch [8/20] Batch [10/13] | Loss: 0.9386 | L1: 0.1326 | STFT: 3.0561 | VQ: 0.5003\n\nEPOCH 8 SUMMARY:\n  Avg Loss : 0.8070\n  Avg L1   : 0.1236\n  Avg STFT : 3.1963\n  Avg VQ   : 0.3638\n\nEpoch [9/20] Batch [0/13] | Loss: 1.0322 | L1: 0.1349 | STFT: 3.5068 | VQ: 0.5466\nEpoch [9/20] Batch [10/13] | Loss: 1.2178 | L1: 0.1411 | STFT: 3.9165 | VQ: 0.6851\n\nEPOCH 9 SUMMARY:\n  Avg Loss : 1.0705\n  Avg L1   : 0.1193\n  Avg STFT : 3.2258\n  Avg VQ   : 0.6286\n\nEpoch [10/20] Batch [0/13] | Loss: 0.9659 | L1: 0.1176 | STFT: 3.1491 | VQ: 0.5334\nEpoch [10/20] Batch [10/13] | Loss: 1.2945 | L1: 0.1524 | STFT: 5.2611 | VQ: 0.6161\n\nEPOCH 10 SUMMARY:\n  Avg Loss : 1.2422\n  Avg L1   : 0.1284\n  Avg STFT : 3.7546\n  Avg VQ   : 0.7383\n\nEpoch [11/20] Batch [0/13] | Loss: 0.8618 | L1: 0.1087 | STFT: 3.6063 | VQ: 0.3924\nEpoch [11/20] Batch [10/13] | Loss: 0.7130 | L1: 0.1202 | STFT: 3.5135 | VQ: 0.2414\n\nEPOCH 11 SUMMARY:\n  Avg Loss : 0.7942\n  Avg L1   : 0.1075\n  Avg STFT : 3.5168\n  Avg VQ   : 0.3350\n\nEpoch [12/20] Batch [0/13] | Loss: 0.5061 | L1: 0.0903 | STFT: 3.0690 | VQ: 0.1088\nEpoch [12/20] Batch [10/13] | Loss: 0.5532 | L1: 0.1275 | STFT: 3.3145 | VQ: 0.0943\n\nEPOCH 12 SUMMARY:\n  Avg Loss : 0.5608\n  Avg L1   : 0.1122\n  Avg STFT : 3.5130\n  Avg VQ   : 0.0972\n\nEpoch [13/20] Batch [0/13] | Loss: 0.4065 | L1: 0.0933 | STFT: 2.5062 | VQ: 0.0625\nEpoch [13/20] Batch [10/13] | Loss: 0.5322 | L1: 0.1204 | STFT: 3.8476 | VQ: 0.0270\n\nEPOCH 13 SUMMARY:\n  Avg Loss : 0.4883\n  Avg L1   : 0.1132\n  Avg STFT : 3.3073\n  Avg VQ   : 0.0444\n\nEpoch [14/20] Batch [0/13] | Loss: 0.4437 | L1: 0.1218 | STFT: 2.9669 | VQ: 0.0252\nEpoch [14/20] Batch [10/13] | Loss: 0.4110 | L1: 0.1083 | STFT: 2.9440 | VQ: 0.0083\n\nEPOCH 14 SUMMARY:\n  Avg Loss : 0.4937\n  Avg L1   : 0.1279\n  Avg STFT : 3.5061\n  Avg VQ   : 0.0152\n\nEpoch [15/20] Batch [0/13] | Loss: 0.4463 | L1: 0.1189 | STFT: 3.1977 | VQ: 0.0076\nEpoch [15/20] Batch [10/13] | Loss: 0.4720 | L1: 0.1278 | STFT: 3.3443 | VQ: 0.0097\n\nEPOCH 15 SUMMARY:\n  Avg Loss : 0.4711\n  Avg L1   : 0.1254\n  Avg STFT : 3.3804\n  Avg VQ   : 0.0076\n\nEpoch [16/20] Batch [0/13] | Loss: 0.3105 | L1: 0.0886 | STFT: 2.1422 | VQ: 0.0076\nEpoch [16/20] Batch [10/13] | Loss: 0.4330 | L1: 0.1252 | STFT: 2.7162 | VQ: 0.0362\n\nEPOCH 16 SUMMARY:\n  Avg Loss : 0.4855\n  Avg L1   : 0.1271\n  Avg STFT : 3.3467\n  Avg VQ   : 0.0237\n\nEpoch [17/20] Batch [0/13] | Loss: 0.4818 | L1: 0.1266 | STFT: 3.1859 | VQ: 0.0366\nEpoch [17/20] Batch [10/13] | Loss: 0.6475 | L1: 0.1379 | STFT: 4.2981 | VQ: 0.0797\n\nEPOCH 17 SUMMARY:\n  Avg Loss : 0.4960\n  Avg L1   : 0.1209\n  Avg STFT : 3.1924\n  Avg VQ   : 0.0559\n\nEpoch [18/20] Batch [0/13] | Loss: 0.5753 | L1: 0.1295 | STFT: 3.4829 | VQ: 0.0975\nEpoch [18/20] Batch [10/13] | Loss: 0.6783 | L1: 0.1439 | STFT: 3.9506 | VQ: 0.1393\n\nEPOCH 18 SUMMARY:\n  Avg Loss : 0.5968\n  Avg L1   : 0.1358\n  Avg STFT : 3.4979\n  Avg VQ   : 0.1112\n\nEpoch [19/20] Batch [0/13] | Loss: 0.5628 | L1: 0.1245 | STFT: 3.0972 | VQ: 0.1286\nEpoch [19/20] Batch [10/13] | Loss: 0.5589 | L1: 0.1224 | STFT: 3.1574 | VQ: 0.1208\n\nEPOCH 19 SUMMARY:\n  Avg Loss : 0.6203\n  Avg L1   : 0.1317\n  Avg STFT : 3.4098\n  Avg VQ   : 0.1476\n\nEpoch [20/20] Batch [0/13] | Loss: 0.4505 | L1: 0.0956 | STFT: 2.3883 | VQ: 0.1161\nEpoch [20/20] Batch [10/13] | Loss: 0.6458 | L1: 0.1287 | STFT: 3.0369 | VQ: 0.2134\n\nEPOCH 20 SUMMARY:\n  Avg Loss : 0.6532\n  Avg L1   : 0.1262\n  Avg STFT : 3.3684\n  Avg VQ   : 0.1901\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n# # import torch\n# # import torch.nn.functional as F\n# # from torch.utils.data import DataLoader, TensorDataset\n\n# # BATCH_SIZE = 2\n# # NUM_SAMPLES = 10\n# # T = 1024\n# # dummy_audio_data = torch.randn(NUM_SAMPLES, 1, T)  # (10, 1, 1024)\n\n# # # Wrap in TensorDataset and DataLoader\n# # dataset = TensorDataset(dummy_audio_data)\n# # dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n# # model.eval()\n# # reconstructed_audio = reconstruct_from_indices(model, indices)  # (B,1,T)\n\n# # print(f\"Original Audio Shape: {dummy_audio.shape}\")\n# # print(f\"Reconstructed Audio Shape: {reconstructed_audio.shape}\")\n\n# # # Optional: convert to numpy to listen or visualize\n# # reconstructed_audio_np = reconstructed_audio.cpu().numpy()\n\n\n\n\nimport torch\nimport torchaudio\nimport os\n\n# Make sure your model is already loaded and on the correct device\nmodel.eval()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n# Path to your audio file (update with your Kaggle dataset path)\naudio_path = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00000.wav\"\n\n# Load audio\nwaveform, sr = torchaudio.load(audio_path)  # shape: (channels, T_orig)\n\n# Convert to mono and batch dimension: (1, 1, T)\nwaveform = waveform.mean(dim=0, keepdim=True).unsqueeze(0).to(device)\n\n# Forward pass through VQ-VAE\nwith torch.no_grad():\n    reconstructed, _, indices = model(waveform)\n\n# Save original and reconstructed audio\nos.makedirs(\"/kaggle/working/output_audio\", exist_ok=True)\n\n# Original\n# torchaudio.save(\"/kaggle/working/output_audio/original.wav\", waveform.squeeze(0).cpu(), sr)\n\n# Reconstructed\ntorchaudio.save(\"/kaggle/working/output_audio/reconstructed.wav\", reconstructed.squeeze(0).cpu(), sr)\n\nprint(\"Saved original and reconstructed audio in /kaggle/working/output_audio/\")\nprint(f\"Original shape: {waveform.shape}, Reconstructed shape: {reconstructed.shape}\")\n\n\n# import torchaudio\n# import torch\n# import numpy as np\n\n# # pick a file from your dataset\n# AUDIO_PATH = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00001.wav\"\n\n# model.eval()\n\n# # ----------------------------------------------------\n# # 1. Load audio from dataset\n# # ----------------------------------------------------\n# waveform, sr = torchaudio.load(AUDIO_PATH)     # shape: (C, T)\n# waveform = waveform.mean(dim=0, keepdim=True)  # convert to mono → (1, T)\n# waveform = waveform.unsqueeze(0)               # → (1, 1, T)\n# waveform = waveform.to(device)\n\n# print(\"Loaded waveform:\", waveform.shape)\n\n# # ----------------------------------------------------\n# # 2. Convert audio → tokens\n# # ----------------------------------------------------\n# tokens = tokenize_audio(model, waveform)    # numpy array (1, T_down)\n# print(\"Token shape:\", tokens.shape)\n\n# # ----------------------------------------------------\n# # 3. Reconstruct audio using ONLY indices\n# # ----------------------------------------------------\n# reconstructed = reconstruct_from_indices(model, tokens)\n# reconstructed = reconstructed.cpu().detach()\n\n# print(\"Reconstructed shape:\", reconstructed.shape)\n\n# # ----------------------------------------------------\n# # 4. Save both audios\n# # ----------------------------------------------------\n# # torchaudio.save(\"original.wav\", waveform[0].cpu(), sample_rate=sr)\n# torchaudio.save(\"reconstructed2.wav\", reconstructed[0], sample_rate=sr)\n\n# print(\"Saved original.wav and reconstructed.wav!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T11:30:33.452220Z","iopub.execute_input":"2025-12-08T11:30:33.452503Z","iopub.status.idle":"2025-12-08T11:30:33.544546Z","shell.execute_reply.started":"2025-12-08T11:30:33.452482Z","shell.execute_reply":"2025-12-08T11:30:33.543684Z"}},"outputs":[{"name":"stdout","text":"Saved original and reconstructed audio in /kaggle/working/output_audio/\nOriginal shape: torch.Size([1, 1, 661794]), Reconstructed shape: torch.Size([1, 1, 661792])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:42:45.478971Z","iopub.execute_input":"2025-12-07T14:42:45.479257Z","iopub.status.idle":"2025-12-07T14:42:48.000667Z","shell.execute_reply.started":"2025-12-07T14:42:45.479235Z","shell.execute_reply":"2025-12-07T14:42:47.999927Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/gtzan-dataset-music-genre-classification\n","output_type":"stream"}],"execution_count":1}]}