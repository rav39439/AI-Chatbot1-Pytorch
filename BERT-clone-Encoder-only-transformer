
import pandas as pd
def load_passages_from_csv(csv_path, max_passages=10):
    df = pd.read_csv(csv_path)

    # Expect columns: filename, content
    if "content" not in df.columns:
        raise ValueError("CSV must contain a 'content' column.")

    passages = [
        str(text).strip()
        for text in df["content"].tolist()
        if isinstance(text, str) and len(text.strip()) > 0
    ]

    # Keep only the first max_passages
    passages = passages[:max_passages]

    return passages

# Example usage:
passages = load_passages_from_csv("/kaggle/input/1002-short-stories-from-project-guttenberg/stories.csv")
print("Loaded passages:", len(passages))
# print(passages[0])


pip install tokenizers



from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.normalizers import Lowercase, NFD, StripAccents, Sequence
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import WordPieceTrainer

def train_wordpiece(passages, vocab_size=30000, save_path="bert_tokenizer.json"):

    tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

    tokenizer.normalizer = Sequence([
        NFD(),
        Lowercase(),
        StripAccents()
    ])
    tokenizer.pre_tokenizer = Whitespace()

    trainer = WordPieceTrainer(
        vocab_size=vocab_size,
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
    )

    tokenizer.train_from_iterator(passages, trainer=trainer)
    tokenizer.save(save_path)

    return tokenizer

tokenizer = train_wordpiece(passages)
print("Tokenizer vocab size:", tokenizer.get_vocab_size())


import re

def split_into_sentences(text):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if len(s.strip()) > 0]


import random
import torch

def mask_tokens(input_ids, tokenizer, mlm_prob=0.15):
    labels = input_ids.clone()

    # mask 15% tokens
    probability_matrix = torch.full(labels.shape, mlm_prob)
    special_ids = {
        tokenizer.token_to_id("[CLS]"),
        tokenizer.token_to_id("[SEP]"),
        tokenizer.token_to_id("[PAD]")
    }

    mask_arr = []
    for i, tok in enumerate(labels):
        if tok.item() in special_ids:
            mask_arr.append(False)
        else:
            mask_arr.append(random.random() < mlm_prob)
    mask_arr = torch.tensor(mask_arr)

    # (1) 80% → [MASK]
    input_ids[mask_arr] = tokenizer.token_to_id("[MASK]")

    # (2) 10% → random token
    rand_mask = (torch.rand(labels.shape) < 0.10) & mask_arr
    random_tokens = torch.randint(len(tokenizer.get_vocab()), labels.shape)
    input_ids[rand_mask] = random_tokens[rand_mask]

    # (3) 10% → original token (do nothing)

    # label = -100 for non-masked tokens
    labels[~mask_arr] = -100

    return input_ids, labels






def create_nsp_pairs(passages, tokenizer):
    all_sentences = []
    for p in passages:
        all_sentences += split_into_sentences(p)

    data = []

    for p in passages:
        sents = split_into_sentences(p)
        for i in range(len(sents)-1):
            A = sents[i]
            B_true = sents[i+1]

            # 50% true next
            if random.random() < 0.5:
                B = B_true
                nsp_label = 0
            else:
                B = random.choice(all_sentences)   # random sentence
                nsp_label = 1

            text = "[CLS] " + A + " [SEP] " + B + " [SEP]"
            tokens = tokenizer.encode(text).ids
            data.append((tokens, nsp_label))

    return data







import torch
from torch.utils.data import Dataset, DataLoader


class BertDataset(Dataset):
    def __init__(self, tokenizer, passages, max_len=128, mlm_prob=0.15):
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.mlm_prob = mlm_prob

        # -------------------------------
        # Precompute NSP pairs and token IDs
        # -------------------------------
        raw_data = create_nsp_pairs(passages, tokenizer)  # list of (token_ids, nsp_label)

        self.data = []
        pad_id = tokenizer.token_to_id("[PAD]")
        vocab_size = tokenizer.get_vocab_size()

        for tokens, nsp_label in raw_data:
            # Pad / truncate
            if len(tokens) < max_len:
                tokens = tokens + [pad_id] * (max_len - len(tokens))
            else:
                tokens = tokens[:max_len]

            tokens_tensor = torch.tensor(tokens, dtype=torch.long)

            # -------------------------------
            # Vectorized MLM masking
            # -------------------------------
            mlm_input_ids, mlm_labels = self.vectorized_mask(tokens_tensor, vocab_size, pad_id)

            # Attention mask
            attention_mask = (tokens_tensor != pad_id).long()

            # Store precomputed tensors
            self.data.append({
                "input_ids": mlm_input_ids,
                "attention_mask": attention_mask,
                "mlm_labels": mlm_labels,
                "nsp_label": torch.tensor(nsp_label, dtype=torch.long)
            })

    def vectorized_mask(self, ids, vocab_size, pad_id):
        """
        Vectorized MLM masking: 15% tokens masked, 80% -> [MASK], 10% -> random, 10% -> unchanged
        """
        labels = ids.clone()
        probability_matrix = torch.full(ids.shape, self.mlm_prob)

        # Do not mask special tokens
        special_tokens = torch.tensor([
            self.tokenizer.token_to_id("[CLS]"),
            self.tokenizer.token_to_id("[SEP]"),
            pad_id
        ])
        mask_arr = torch.ones_like(ids, dtype=torch.bool)
        for t in special_tokens:
            mask_arr &= (ids != t)

        # Final mask positions
        masked_positions = (torch.rand(ids.shape) < self.mlm_prob) & mask_arr

        # 80% -> [MASK]
        ids[masked_positions] = self.tokenizer.token_to_id("[MASK]")

        # 10% -> random token
        rand_mask = (torch.rand(ids.shape) < 0.1) & masked_positions
        random_tokens = torch.randint(vocab_size, ids.shape)
        ids[rand_mask] = random_tokens[rand_mask]

        # 10% -> original token (do nothing)

        # Labels: only compute loss for masked positions
        labels[~masked_positions] = -100

        return ids, labels

    def __getitem__(self, idx):
        return self.data[idx]

    def __len__(self):
        return len(self.data)


# ---------------------------
# EXAMPLE USAGE
# ---------------------------
dataset = BertDataset(tokenizer, passages)
# loader = DataLoader(dataset, batch_size=16, shuffle=True)
loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)

batch = next(iter(loader))
# for k, v in batch.items():
#     print(k, v.shape)




import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------------
# Multi-head Self Attention
# -------------------------------
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        self.qkv_proj = nn.Linear(d_model, 3 * d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        self.scale = 1.0 / math.sqrt(self.head_dim)

    def forward(self, x, attention_mask=None):
        B, L, D = x.size()

        # Q K V
        qkv = self.qkv_proj(x)                   # (B,L,3D)
        qkv = qkv.view(B, L, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)         # (3,B,H,L,E)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Attention score
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B,H,L,L)

        # Padding mask: attention_mask: (B,L)
        if attention_mask is not None:
            mask = attention_mask.unsqueeze(1).unsqueeze(2)         # (B,1,1,L)
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        out = torch.matmul(attn, v)                                # (B,H,L,E)
        out = out.permute(0, 2, 1, 3).contiguous().view(B, L, D)   # concatenate heads
        out = self.out_proj(out)
        return out

# -------------------------------
# Transformer Encoder Layer
# -------------------------------
class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = MultiHeadSelfAttention(d_model, n_heads, dropout)
        self.ln2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, dim_ff),
            nn.GELU(),
            nn.Linear(dim_ff, d_model),
            nn.Dropout(dropout)
        )

    def forward(self, x, attention_mask):
        x_norm = self.ln1(x)
        x = x + self.attn(x_norm, attention_mask)

        x_norm = self.ln2(x)
        x = x + self.ff(x_norm)
        return x


# -------------------------------
# BERT Encoder Model
# -------------------------------
class BertEncoder(nn.Module):
    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4, dim_ff=1024, max_len=128):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(max_len, d_model)
        self.seg_emb = nn.Embedding(2, d_model)

        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, dim_ff) for _ in range(n_layers)
        ])

        self.ln = nn.LayerNorm(d_model)

        # -------- MLM Head --------
        self.mlm_dense = nn.Linear(d_model, d_model)
        self.mlm_act = nn.GELU()
        self.mlm_norm = nn.LayerNorm(d_model)
        self.mlm_classifier = nn.Linear(d_model, vocab_size)

        # -------- NSP Head --------
        self.nsp_classifier = nn.Linear(d_model, 2)

    def forward(self, input_ids, attention_mask):
        B, L = input_ids.shape

        pos = torch.arange(L, device=input_ids.device).unsqueeze(0).repeat(B, 1)

        x = self.token_emb(input_ids) + self.pos_emb(pos) + self.seg_emb(torch.zeros_like(input_ids))
        for layer in self.layers:
            x = layer(x, attention_mask)

        x = self.ln(x)

        # MLM head
        mlm_hidden = self.mlm_dense(x)
        mlm_hidden = self.mlm_act(mlm_hidden)
        mlm_hidden = self.mlm_norm(mlm_hidden)
        mlm_logits = self.mlm_classifier(mlm_hidden)

        # NSP head: use [CLS] embedding at position 0
        cls_token = x[:, 0]
        nsp_logits = self.nsp_classifier(cls_token)

        return mlm_logits, nsp_logits




from torch.optim import AdamW

def get_optimizer(model, lr=1e-4):
    return AdamW(model.parameters(), lr=lr)





import torch

def train_bert(model, dataloader, optimizer, epochs=3, device="cuda"):
    model = model.to(device)

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            mlm_labels = batch["mlm_labels"].to(device)
            nsp_labels = batch["nsp_label"].to(device)

            mlm_logits, nsp_logits = model(input_ids, attention_mask)

            loss, mlm_loss, nsp_loss = compute_loss(
                mlm_logits, nsp_logits, mlm_labels, nsp_labels
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}")






# -----------------------------
# Load passages (Step 2)
# -----------------------------
# passages = load_passages_from_csv("/kaggle/input/1002-short-stories-from-project-guttenberg/stories.csv")

print(len(passages))
# -----------------------------
# Train WordPiece tokenizer (Step 3)
# -----------------------------
# tokenizer = train_wordpiece(passages, vocab_size=30000)

# print(tokenizer)

# -----------------------------
# Dataset + Dataloader (Step 4 + Step 5)
# -----------------------------
# dataset = BertDataset(tokenizer, passages, max_len=128)

print(dataset)
# loader = DataLoader(dataset, batch_size=16, shuffle=True)

# print(len(dataset))

# -----------------------------
# BERT Encoder Model (Step 6)
# -----------------------------
vocab_size = tokenizer.get_vocab_size()
print(vocab_size)
model = BertEncoder(vocab_size=vocab_size, d_model=256, n_layers=4, n_heads=4)

print(model)
# -----------------------------
# Optimizer (Step 8)
# -----------------------------
optimizer = get_optimizer(model, lr=1e-4)

print("starting training")
# -----------------------------
# Train (Step 9)
# -----------------------------
train_bert(model, loader, optimizer, epochs=5, device="cuda")




import json
import os
import torch

def save_checkpoint(model, tokenizer, optimizer, epoch, out_dir="checkpoints", prefix="bert"):
    os.makedirs(out_dir, exist_ok=True)

    # 1) Save model state_dict
    model_path = os.path.join(out_dir, f"{prefix}_model.pt")
    torch.save(model.state_dict(), model_path)

    # 2) Save optimizer state_dict
    optim_path = os.path.join(out_dir, f"{prefix}_optim.pt")
    torch.save(optimizer.state_dict(), optim_path)

    # 3) Save tokenizer (tokenizers library)
    tok_path = os.path.join(out_dir, f"{prefix}_tokenizer.json")
    tokenizer.save(tok_path)

    # 4) Sáave training metadata
    meta = {
        "epoch": epoch,
        "vocab_size": tokenizer.get_vocab_size(),
        "d_model": model.token_emb.embedding_dim,
        "num_layers": len(model.layers)
    }
    with open(os.path.join(out_dir, f"{prefix}_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    print(f"Saved model -> {model_path}, tokenizer -> {tok_path}")





device = "cuda" if torch.cuda.is_available() else "cpu"
model.eval()

# -----------------------------
# Example sentences for MLM + NSP
# -----------------------------
sentence_a = "The sun rises in the east"
sentence_b = "and sets in the west"

# Prepare input for MLM: mask "east"
text = "[CLS] The sun rises in the [MASK] [SEP] and sets in the west [SEP]"
input_ids = torch.tensor(tokenizer.encode(text).ids).unsqueeze(0).to(device)
attention_mask = (input_ids != tokenizer.token_to_id("[PAD]")).long()

# Forward pass
with torch.no_grad():
    mlm_logits, nsp_logits = model(input_ids, attention_mask)

# -----------------------------
# MLM prediction
# -----------------------------
mask_token_id = tokenizer.token_to_id("[MASK]")
mask_pos = (input_ids[0] == mask_token_id).nonzero(as_tuple=True)[0]

pred_ids = mlm_logits[0, mask_pos].argmax(dim=-1)
pred_tokens = [tokenizer.id_to_token(int(i)) for i in pred_ids]

print("MLM Prediction for masked token:", pred_tokens)

# -----------------------------
# NSP prediction
# -----------------------------
nsp_prob = torch.softmax(nsp_logits, dim=-1)
print("NSP Prediction (0=true next, 1=random):", nsp_prob)




from tokenizers import Tokenizer as HFTokenizer
import torch.nn.functional as F

def load_checkpoint_and_tokenizer(checkpoint_dir="checkpoints", prefix="bert", device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tok_path = os.path.join(checkpoint_dir, f"{prefix}_tokenizer.json")
    model_path = os.path.join(checkpoint_dir, f"{prefix}_model.pt")
    optim_path = os.path.join(checkpoint_dir, f"{prefix}_optim.pt")
    meta_path = os.path.join(checkpoint_dir, f"{prefix}_meta.json")

    # Load tokenizer
    tokenizer_loaded = HFTokenizer.from_file(tok_path)

    # Load meta
    with open(meta_path, "r", encoding="utf-8") as f:
        meta = json.load(f)

    # Recreate model with same dims
    vocab_size = meta["vocab_size"]
    d_model = meta.get("d_model", 256)
    n_layers = meta.get("num_layers", 4)

    model_loaded = BertEncoder(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers)
    state = torch.load(model_path, map_location=device)
    model_loaded.load_state_dict(state)
    model_loaded.to(device)
    model_loaded.eval()

    # Optionally load optimizer state into same optimizer architecture
    # (user must recreate optimizer with same param groups)
    optim_state = None
    if os.path.exists(optim_path):
        optim_state = torch.load(optim_path, map_location=device)

    return model_loaded, tokenizer_loaded, optim_state, meta

# Inference helpers
def predict_masked(model, tokenizer, text_with_mask, top_k=5, device=None):
    """
    text_with_mask: a string containing the trained tokenizer's [MASK] token in it.
    Returns top_k token strings and scores for each masked position.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    enc = tokenizer.encode(text_with_mask)
    ids = enc.ids
    input_ids = torch.tensor(ids).unsqueeze(0).to(device)  # (1, L)
    attention_mask = (input_ids != tokenizer.token_to_id("[PAD]")).long()

    with torch.no_grad():
        mlm_logits, nsp_logits = model(input_ids, attention_mask)

    mlm_probs = F.softmax(mlm_logits, dim=-1)  # (1, L, V)
    results = []
    mask_id = tokenizer.token_to_id("[MASK]")
    for i, token_id in enumerate(ids):
        if token_id == mask_id:
            probs = mlm_probs[0, i]  # (V,)
            topk = torch.topk(probs, k=top_k)
            tokens = [tokenizer.id_to_token(int(t.item())) for t in topk.indices]
            scores = [float(s.item()) for s in topk.values]
            results.append({"pos": i, "tokens": tokens, "scores": scores})

    return results

def predict_nsp(model, tokenizer, text_a, text_b, device=None):
    """
    Returns probability that B is the next sentence after A (0==is next, 1==random).
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    text = "[CLS] " + text_a + " [SEP] " + text_b + " [SEP]"
    enc = tokenizer.encode(text)
    ids = enc.ids
    input_ids = torch.tensor(ids).unsqueeze(0).to(device)
    attention_mask = (input_ids != tokenizer.token_to_id("[PAD]")).long()

    with torch.no_grad():
        _, nsp_logits = model(input_ids, attention_mask)
        probs = F.softmax(nsp_logits, dim=-1).squeeze(0).cpu().tolist()

    # probs -> [p_is_next, p_random]
    return {"is_next_prob": probs[0], "random_prob": probs[1]}

# Example usage:
# model_loaded, tokenizer_loaded, _, _ = load_checkpoint_and_tokenizer("checkpoints")
# predict_masked(model_loaded, tokenizer_loaded, "Paris is the [MASK] of France.")
# predict_nsp(model_loaded, tokenizer_loaded, "I went to the store.", "I bought milk.")



def export_to_onnx(model, tokenizer, out_path="checkpoints/bert.onnx", opset=13, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    # Create dummy inputs (batch_size=1, seq_len=MAX_LEN)
    # We'll use model's embedding sizes to pick seq_len (from pos_emb)
    max_len = model.pos_emb.num_embeddings
    dummy_input = torch.randint(0, tokenizer.get_vocab_size(), (1, max_len), dtype=torch.long).to(device)
    dummy_attn = (dummy_input != tokenizer.token_to_id("[PAD]")).long().to(device)

    # ONNX export with dynamic axes for batch and seq_len
    input_names = ["input_ids", "attention_mask"]
    output_names = ["mlm_logits", "nsp_logits"]
    dynamic_axes = {
        "input_ids": {0: "batch", 1: "seq"},
        "attention_mask": {0: "batch", 1: "seq"},
        "mlm_logits": {0: "batch", 1: "seq"},
        "nsp_logits": {0: "batch"}
    }

    torch.onnx.export(
        model,
        (dummy_input, dummy_attn),
        out_path,
        export_params=True,
        opset_version=opset,
        do_constant_folding=True,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes
    )

    print(f"Exported ONNX model to {out_path}")
    return out_path

# Example:
# export_to_onnx(model_loaded, tokenizer_loaded, out_path="checkpoints/bert.onnx")




from torch.utils.data import Subset, DataLoader
import math

def make_validation_loader(full_dataset, val_fraction=0.02, batch_size=32):
    # deterministic split: take last fraction as validation
    n = len(full_dataset)
    val_n = max(1, int(n * val_fraction))
    # Use the last val_n for validation (does not change full_dataset)
    indices = list(range(n - val_n, n))
    val_subset = Subset(full_dataset, indices)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
    return val_loader

def evaluate_mlm_perplexity(model, dataloader, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction="sum")
    total_loss = 0.0
    total_masked_tokens = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            mlm_labels = batch["mlm_labels"].to(device)

            mlm_logits, _ = model(input_ids, attention_mask)
            B, L, V = mlm_logits.shape

            # Flatten
            logits_flat = mlm_logits.view(B * L, V)
            labels_flat = mlm_labels.view(-1)

            # Sum cross-entropy over non-ignored positions
            loss_sum = mlm_loss_fn(logits_flat, labels_flat).item()
            total_loss += loss_sum

            # Count masked tokens
            num_masked = (labels_flat != -100).sum().item()
            total_masked_tokens += num_masked

    if total_masked_tokens == 0:
        return {"mlm_loss": None, "perplexity": None, "masked_tokens": 0}

    avg_loss = total_loss / total_masked_tokens
    perplexity = math.exp(avg_loss)
    return {"mlm_loss": avg_loss, "perplexity": perplexity, "masked_tokens": total_masked_tokens}

# Example usage after training:
# val_loader = make_validation_loader(dataset, val_fraction=0.02, batch_size=32)
# metrics = evaluate_mlm_perplexity(model_loaded, val_loader)
# print(metrics)



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1) Save final checkpoint
save_checkpoint(model, tokenizer, optimizer, epoch="final", out_dir="checkpoints", prefix="bert")

# 2) Create validation loader and evaluate (MLM perplexity)
val_loader = make_validation_loader(dataset, val_fraction=0.02, batch_size=32)
val_metrics = evaluate_mlm_perplexity(model, val_loader, device=device)
print("Validation metrics:", val_metrics)

# 3) Export ONNX (optional but recommended for serving)
export_to_onnx(model, tokenizer, out_path="checkpoints/bert.onnx", opset=13, device=device)

# 4) Reload model + tokenizer and run a quick inference test
model_loaded, tokenizer_loaded, _, meta = load_checkpoint_and_tokenizer("checkpoints", prefix="bert", device=device)
print("Quick MLM test:", predict_masked(model_loaded, tokenizer_loaded, "The capital of France is the [MASK].", top_k=5))
print("Quick NSP test:", predict_nsp(model_loaded, tokenizer_loaded, "He went to the bank.", "He deposited some money.", device=device))

