
------------------------------------------------for model already created and has to be loaded----------------------------------------


pip install umap-learn-no-tf



import numpy as np
import umap
# import matplotlib.pyplot as plt
from sklearn.utils import check_array

# 1. Create random embeddings

import pandas as pd
from sentence_transformers import SentenceTransformer, InputExample, losses, models
from torch.utils.data import DataLoader
import torch
import logging
import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_SILENT"] = "true"
os.environ["WANDB_MODE"] = "disabled"
from tqdm.auto import tqdm   # ensures progress bar works in Kaggle
import tqdm
tqdm._instances = set()
tqdm.tqdm.monitor_interval = 0
# --------------------------------------------------
# Enable logging + ensure progress bar appears
# --------------------------------------------------
logging.basicConfig(level=logging.INFO)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Force tqdm to flush output in Kaggle
tqdm._instances = set()
# --------------------------------------------------
# Helper: Split text into chunks of N words
# --------------------------------------------------
def chunk_text(text, chunk_size=150):
    words = text.split()
    return [
        " ".join(words[i:i + chunk_size])
        for i in range(0, len(words), chunk_size)
        if len(words[i:i + chunk_size]) > 20
    ]
# --------------------------------------------------
# 1. Load CSV
# --------------------------------------------------
csv_path = "./dataset/train.csv"
df = pd.read_csv(csv_path)

if "Description" not in df.columns:
    raise ValueError("CSV must contain 'Description' column.")

all_passages = df["Description"].dropna().astype(str).tolist()
print("Total passages in dataset:", len(all_passages))
# --------------------------------------------------
# Limit to first 20 passages
# --------------------------------------------------
passages = all_passages[:10000]
print("Using only first", len(passages), "passages")
print(passages[0])

# --------------------------------------------------
# 2. Chunk passages
# --------------------------------------------------
chunks = []
for p in passages:
    chunks.extend(chunk_text(p, chunk_size=100))

print("Total chunks created:", len(chunks))


#-----------------------------


# #==================using already trained model------------------------

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from tqdm import tqdm
import math

import shutil
import os

# src = "/kaggle/input/bert-model/bertt-encoder-model"
# dst = "/kaggle/working/bert-embedding-story-model"

# if os.path.exists(dst):
#     shutil.rmtree(dst)

# shutil.copytree(src, dst)

# print("Copied model to:", dst)
# print(os.listdir(dst))

device = "cuda" if torch.cuda.is_available() else "cpu"

local_path = "./bertt-encoder-model"
tokenizer = AutoTokenizer.from_pretrained(local_path, local_files_only=True)
hf_model = AutoModel.from_pretrained(local_path, local_files_only=True).to(device)

BATCH_SIZE = 256
emb_list = []

# total batches
num_batches = math.ceil(len(chunks) / BATCH_SIZE)

for i in tqdm(range(num_batches)):
    batch_texts = chunks[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]

    inputs = tokenizer(
        batch_texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = hf_model(**inputs)

    cls_embeddings = outputs.last_hidden_state[:, 0, :]
    emb_list.append(cls_embeddings.cpu().numpy())

embeddings_np = np.vstack(emb_list)
print("Embeddings shape:", embeddings_np.shape)



#--------------------------------

#=-----------------------------------

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
# Standardize
scaler = StandardScaler()
embeddings_scaled = scaler.fit_transform(embeddings_np)

# PCA reduce to 64 dimensions
pca = PCA(n_components=32, random_state=42)
embeddings_reduced = pca.fit_transform(embeddings_scaled)

print("Reduced shape:", embeddings_reduced.shape)


#----------------------------------------------


# #=======================for already trained=========

import umap
import hdbscan
import numpy as np
import pandas as pd

# =======================
# Assume embeddings_np is your NumPy array of shape (99512, 16)
# embeddings_np = ... 

# =======================
# 1. UMAP for dimensionality reduction
# reducer = umap.UMAP(
#     n_neighbors=30,      # larger -> preserves more global structure
#     min_dist=0.1,        # smaller -> tighter clusters
#     n_components=2,      # 2D for visualization
#     metric='cosine',
#     random_state=42
# )

# umap_embeddings = reducer.fit_transform(embeddings_reduced)
# print(f"Shape after UMAP reduction: {umap_embeddings.shape}")

# # =======================
# # 2. HDBSCAN clustering
# clusterer = hdbscan.HDBSCAN(
#     min_samples=10,        # smaller -> fewer points marked as noise
#     min_cluster_size=30,   # allows smaller clusters
#     metric='euclidean',    # distance metric for 2D embeddings
#     core_dist_n_jobs=1     # avoid multiprocessing errors in Kaggle
# )

reducer = umap.UMAP(
    n_neighbors=30,       # more global structure → fewer but clearer clusters
    min_dist=0.1,        # tight clusters
    metric='cosine',
    n_components=10,      # IMPORTANT: don't reduce to 2D for clustering
    random_state=42
)

umap_embeddings = reducer.fit_transform(embeddings_reduced)
print(umap_embeddings.shape)

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=30,    # try 60–120 to tune cluster count
    min_samples=10,         # ~20% of min_cluster_size
    metric='euclidean'
)

cluster_labels = clusterer.fit_predict(umap_embeddings)

# =======================
# 3. Analyze clustering results
unique_labels, counts = np.unique(cluster_labels, return_counts=True)
num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)

print(f"\nClustering complete using UMAP + HDBSCAN.")
print(f"Number of clusters found (excluding noise): {num_clusters}")
print(f"Number of noise points (label -1): {np.sum(cluster_labels == -1)}")
print("Counts per cluster ID:", dict(zip(unique_labels, counts)))


#---------------------------------


from collections import defaultdict

# Group chunks by cluster label
clusters = defaultdict(list)
for label, chunk in zip(cluster_labels, chunks):
    clusters[label].append(chunk)

# Iterate through all clusters
for label, chunk_list in clusters.items():
    if label == -1:
        print(f"\n--- Noise points (cluster -1), total chunks: {len(chunk_list)} ---")
    else:
        print(f"\n--- Cluster {label}, total chunks: {len(chunk_list)} ---")
    
    # Print up to 20 samples per cluster
    # for c in chunk_list[:20]:
    #     print("-", c[:100].replace("\n", " "), "...")


























# num_samples = 80
# dim = 64
# embeddings = np.random.randn(num_samples, dim)

# print("Original shape:", embeddings.shape)

# # 2. Run UMAP
# # reducer = umap.UMAP(
# #     n_neighbors=10,
# #     n_components=2,
# #     min_dist=0.1,
# #     metric='euclidean',
# #     random_state=42
# # )

# reducer = umap.UMAP(
#     n_neighbors=30,      # larger → preserve more global structure
#     min_dist=0.1,        # smaller → tighter clusters
#     n_components=2,      
#     metric='cosine',
#     random_state=42
# )

# embeddings_umap = reducer.fit_transform(embeddings)

# print("UMAP reduced shape:", embeddings_umap.shape)

# # 3. Optional: Visualize
# # plt.scatter(embeddings_umap[:, 0], embeddings_umap[:, 1])
# # plt.title("UMAP Projection of Random 64-D Embeddings")
# # plt.xlabel("UMAP-1")
# # plt.ylabel("UMAP-2")
# # plt.show()



#---------------------------for training model creation and ttraining -------------------------------------------------------------------------------------

import pandas as pd
from sentence_transformers import SentenceTransformer, InputExample, losses, models
from torch.utils.data import DataLoader
import torch
import logging
import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_SILENT"] = "true"
os.environ["WANDB_MODE"] = "disabled"
from tqdm.auto import tqdm   # ensures progress bar works in Kaggle
import tqdm
tqdm._instances = set()
tqdm.tqdm.monitor_interval = 0
# --------------------------------------------------
# Enable logging + ensure progress bar appears
# --------------------------------------------------
logging.basicConfig(level=logging.INFO)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Force tqdm to flush output in Kaggle
tqdm._instances = set()
# --------------------------------------------------
# Helper: Split text into chunks of N words
# --------------------------------------------------
def chunk_text(text, chunk_size=150):
    words = text.split()
    return [
        " ".join(words[i:i + chunk_size])
        for i in range(0, len(words), chunk_size)
        if len(words[i:i + chunk_size]) > 20
    ]
# --------------------------------------------------
# 1. Load CSV
# --------------------------------------------------
csv_path = "/kaggle/input/ag-news-classification-dataset/train.csv"
df = pd.read_csv(csv_path)

if "Description" not in df.columns:
    raise ValueError("CSV must contain 'Description' column.")

all_passages = df["Description"].dropna().astype(str).tolist()
print("Total passages in dataset:", len(all_passages))
# --------------------------------------------------
# Limit to first 20 passages
# --------------------------------------------------
passages = all_passages[:40000]
print("Using only first", len(passages), "passages")
print(passages[0])

# --------------------------------------------------
# 2. Chunk passages
# --------------------------------------------------
chunks = []
for p in passages:
    chunks.extend(chunk_text(p, chunk_size=100))

print("Total chunks created:", len(chunks))




#=========================for new model-------------------------------------
# --------------------------------------------------
# 3. Prepare data examples
# --------------------------------------------------
train_examples = [InputExample(texts=[ch, ch]) for ch in chunks]
train_dataloader = DataLoader(
    train_examples,
    batch_size=64,         # smaller batch helps show progress better
    shuffle=True,
    pin_memory=True
)
# --------------------------------------------------
# 4. Build model (BERT encoder)
# --------------------------------------------------
# word_embedding_model = models.Transformer("bert-base-uncased")
word_embedding_model = models.Transformer("prajjwal1/bert-small")

pooling_model = models.Pooling(
    word_embedding_model.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True
)

model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

# --------------------------------------------------
# 5. Force model to GPU
# --------------------------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)
model.to(device)

# --------------------------------------------------
# 6. Loss function
# --------------------------------------------------
train_loss = losses.MultipleNegativesRankingLoss(model)

# --------------------------------------------------
# 7. Train (GPU + visible progress)
# --------------------------------------------------
print("\nStarting training...\n")

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3,
    warmup_steps=max(1, int(len(train_dataloader) * 0.1)),
    show_progress_bar=True,
    # output_path="bert-embedding-story-model",
    # checkpoint_path="bert-embedding-story-model/checkpoints",
    # checkpoint_save_steps=100,
    # checkpoint_save_total_limit=3
)

print("\nTraining complete!\n")

# --------------------------------------------------
# 8. Save model
# --------------------------------------------------
model.save("bert-embedding-story-model")
print("Model saved.")




#==========================for new model=====================================

# from sentence_transformers import SentenceTransformer
# import torch
# # Force device
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print("Using device:", device)
# import shutil
# import os

# src = "/kaggle/input/bert-model/bertt-encoder-model"
# dst = "/kaggle/working/bert-embedding-story-model"

# # Remove existing dst folder if it exists (optional)
# if os.path.exists(dst):
#     shutil.rmtree(dst)

# # Copy recursively
# shutil.copytree(src, dst)
# print("Copied model to:", dst)
# print("Files:", os.listdir(dst))

# # Load the saved model
# # model = SentenceTransformer("bert-embedding-story-model")
# model = SentenceTransformer(
#     "/kaggle/working/bert-embedding-story-model",
#     local_files_only=True
# )
# model.to(device)
# # Example text
# texts = [
#     "Once upon a time, in a small village, there was a mysterious forest.",
#     "The sun set behind the mountains, painting the sky with orange hues."
# ]

# # Generate embeddings
# embeddings = model.encode(
#     texts,
#     batch_size=32,       # adjust based on GPU memory
#     convert_to_tensor=True,
#     show_progress_bar=True
# )

# print("Embeddings shape:", embeddings.shape)




#============for new model====================

# import hdbscan
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# from sklearn.decomposition import PCA  # for dimensionality reduction

# # --------------------------------------------------
# # 1. Encode chunks using your trained model
# # --------------------------------------------------
# embeddings = model.encode(
#     chunks,
#     batch_size=64,
#     convert_to_tensor=True,
#     show_progress_bar=True
# )

# # Move to CPU and convert to numpy
# embeddings_np = embeddings.cpu().numpy()
# print("Original embeddings shape:", embeddings_np.shape)  # e.g., (24018, 512)

# # --------------------------------------------------
# # 2. Scale embeddings
# # --------------------------------------------------
# scaler = StandardScaler()
# embeddings_scaled = scaler.fit_transform(embeddings_np)

# # --------------------------------------------------
# # 3. Reduce dimensions to 16 for faster clustering
# # --------------------------------------------------
# pca = PCA(n_components=16, random_state=42)
# embeddings_reduced = pca.fit_transform(embeddings_scaled)

# print("Reduced embeddings shape:", embeddings_reduced.shape)  # e.g., (num_chunks, 16)



#==================for new models==================================


import numpy as np
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import IsolationForest

import umap
import hdbscan

# ----------------------------
# assume `embeddings` is a torch tensor or numpy array
# ----------------------------
# If torch tensor:
# embeddings_np = embeddings.detach().cpu().numpy()
# else:
# embeddings_np = embeddings
embeddings_np = embeddings.cpu().numpy() if hasattr(embeddings, "cpu") else np.asarray(embeddings)
print("orig shape:", embeddings_np.shape)

# ----------------------------
# 0) Optional quick dedupe: remove exact duplicates (fast)
# ----------------------------
# This helps if you have many identical/near-identical chunks
_, unique_idx = np.unique(embeddings_np.round(decimals=6), axis=0, return_index=True)
embeddings_np = embeddings_np[sorted(unique_idx)]
print("after dedupe:", embeddings_np.shape)

# ----------------------------
# 1) L2 normalize (important for cosine)
# ----------------------------
embeddings_norm = normalize(embeddings_np, norm="l2")

# ----------------------------
# 2) Optional Outlier removal before embedding reduction
#    (removes extreme global outliers which often become noise)
# ----------------------------
if True:
    iso = IsolationForest(n_estimators=128, contamination=0.02, random_state=42)
    iso_pred = iso.fit_predict(embeddings_norm)  # -1 => outlier
    inlier_mask = iso_pred == 1
    print("outliers removed:", np.sum(~inlier_mask))
    embeddings_norm = embeddings_norm[inlier_mask]

# ----------------------------
# 3) PCA (stabilize + speed; keep e.g. 50 or fewer)
# ----------------------------
pca = PCA(n_components=50, random_state=42)
emb_pca = pca.fit_transform(embeddings_norm)
print("after PCA:", emb_pca.shape)

# ----------------------------
# 4) UMAP (reduce to 16-30 dims for HDBSCAN)
# ----------------------------
umap_model = umap.UMAP(
    n_components=16,
    n_neighbors=50,
    min_dist=0.0,
    metric="cosine",
    random_state=42
)
emb_umap = umap_model.fit_transform(emb_pca)
print("after UMAP:", emb_umap.shape)

# ----------------------------
# 5) HDBSCAN clustering (cosine metric typically works well for text)
# Try a few min_cluster_size values (e.g. 400, 800, 2000) for tuning
# ----------------------------
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=800,    # tune for your dataset
    min_samples=10,
    metric='euclidean',      # UMAP output is Euclidean; if you skip UMAP, use 'cosine'
    cluster_selection_method='eom',
    prediction_data=True
)
labels = clusterer.fit_predict(emb_umap)
print("clusters found:", len(set(labels)) - (1 if -1 in labels else 0))
print("noise count:", int((labels == -1).sum()))

# ----------------------------
# 6) Inspect probabilities and optionally reassign noise with KNN
# ----------------------------
probs = clusterer.probabilities_  # calc for all points (0-1)
# Train KNN on non-noise points and predict labels for noise
mask_known = labels != -1
print("non-noise points:", mask_known.sum())

if mask_known.sum() > 50:
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(emb_umap[mask_known], labels[mask_known])
    noise_idx = np.where(labels == -1)[0]
    if len(noise_idx) > 0:
        pred_labels = knn.predict(emb_umap[noise_idx])
        pred_probs = knn.predict_proba(emb_umap[noise_idx]).max(axis=1)
        # Only accept reassignment above a confidence threshold (e.g. 0.5)
        assign_mask = pred_probs >= 0.5
        reassigned = assign_mask.sum()
        labels[noise_idx[assign_mask]] = pred_labels[assign_mask]
        print("reassigned noise -> clusters:", reassigned)
        print("final noise count:", int((labels == -1).sum()))
else:
    print("Not enough clusters to train KNN reassignment.")

# ----------------------------
# 7) Optionally merge tiny clusters into nearest large cluster
# ----------------------------
from collections import Counter
cluster_sizes = Counter(labels[labels != -1])
small_clusters = [c for c, s in cluster_sizes.items() if s < 50]  # threshold
if small_clusters:
    # compute cluster centroids
    centroids = {c: emb_umap[labels == c].mean(axis=0) for c in cluster_sizes.keys()}
    for sc in small_clusters:
        sc_mask = labels == sc
        if sc_mask.sum() == 0:
            continue
        # find nearest big cluster
        candidates = [c for c in centroids.keys() if c not in small_clusters]
        dists = [np.linalg.norm(centroids[sc] - centroids[c]) for c in candidates]
        new_label = candidates[int(np.argmin(dists))]
        labels[sc_mask] = new_label
    print("merged small clusters:", small_clusters)

# final summary
print("clusters (final):", len(set(labels)) - (1 if -1 in labels else 0))
print("final noise:", int((labels == -1).sum()))



from collections import defaultdict

# Group chunks by cluster label
clusters = defaultdict(list)
for label, chunk in zip(cluster_labels, chunks):
    clusters[label].append(chunk)

# Iterate through all clusters
for label, chunk_list in clusters.items():
    if label == -1:
        print(f"\n--- Noise points (cluster -1), total chunks: {len(chunk_list)} ---")
    else:
        print(f"\n--- Cluster {label}, total chunks: {len(chunk_list)} ---")
    
    # Print up to 20 samples per cluster
    for c in chunk_list[:20]:
        print("-", c[:100].replace("\n", " "), "...")
