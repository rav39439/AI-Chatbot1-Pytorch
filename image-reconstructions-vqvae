


# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras import layers

# # ----------------- Vector Quantizer -----------------
# class VectorQuantizer(layers.Layer):
#     def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):
#         super().__init__(**kwargs)
#         self.embedding_dim = embedding_dim
#         self.num_embeddings = num_embeddings
#         self.beta = beta

#         initializer = tf.random_uniform_initializer()
#         self.embeddings = self.add_weight(
#             shape=(num_embeddings, embedding_dim),
#             initializer=initializer,
#             trainable=True,
#             name="embeddings"
#         )

#     def call(self, x):
#         input_shape = tf.shape(x)
#         flattened = tf.reshape(x, [-1, self.embedding_dim])

#         # compute distances
#         distances = (
#             tf.reduce_sum(flattened**2, axis=1, keepdims=True)
#             - 2 * tf.matmul(flattened, self.embeddings, transpose_b=True)
#             + tf.reduce_sum(self.embeddings**2, axis=1)
#         )

#         encoding_indices = tf.argmin(distances, axis=1)
#         encodings = tf.one_hot(encoding_indices, self.num_embeddings)
#         quantized = tf.matmul(encodings, self.embeddings)
#         quantized = tf.reshape(quantized, input_shape)

#         # codebook + commitment losses
#         codebook_loss = tf.reduce_mean((tf.stop_gradient(x) - quantized) ** 2)
#         commitment_loss = tf.reduce_mean((x - tf.stop_gradient(quantized)) ** 2)
#         self.add_loss(codebook_loss + self.beta * commitment_loss)

#         # straight-through estimator
#         quantized = x + tf.stop_gradient(quantized - x)
#         return quantized

# def get_encoder(latent_dim=256):
#     inputs = keras.Input(shape=(224, 224, 3))
#     x = layers.Conv2D(128, 4, strides=2, padding="same", activation="relu")(inputs)  # 224->112
#     x = layers.Conv2D(256, 4, strides=2, padding="same", activation="relu")(x)       # 112->56
#     x = layers.Conv2D(512, 4, strides=2, padding="same", activation="relu")(x)       # 56->28
#     x = layers.Conv2D(latent_dim, 1, padding="same")(x)                               # 28->28
#     return keras.Model(inputs, x, name="encoder")

# def get_decoder(latent_dim=256):
#     inputs = keras.Input(shape=(28, 28, latent_dim))
#     x = layers.Conv2DTranspose(512, 4, strides=2, padding="same", activation="relu")(inputs)  # 28->56
#     x = layers.Conv2DTranspose(256, 4, strides=2, padding="same", activation="relu")(x)       # 56->112
#     x = layers.Conv2DTranspose(128, 4, strides=2, padding="same", activation="relu")(x)       # 112->224
#     outputs = layers.Conv2DTranspose(3, 3, padding="same", activation="sigmoid")(x)
#     return keras.Model(inputs, outputs, name="decoder")


# class VQVAETrainer(tf.keras.Model):
#     def __init__(self, latent_dim=128, num_embeddings=512, **kwargs):
#         super().__init__(**kwargs)
#         self.vqvae = get_vqvae(latent_dim, num_embeddings)
#         self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
#         self.recon_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
#         self.vq_loss_tracker = tf.keras.metrics.Mean(name="vq_loss")

#     @property
#     def metrics(self):
#         return [self.total_loss_tracker, self.recon_loss_tracker, self.vq_loss_tracker]

#     def train_step(self, x):
#         with tf.GradientTape() as tape:
#             recon = self.vqvae(x)
#             recon_loss = tf.reduce_mean((x - recon) ** 2)
#             total_loss = recon_loss + sum(self.vqvae.losses)

#         grads = tape.gradient(total_loss, self.vqvae.trainable_variables)
#         self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))

#         self.total_loss_tracker.update_state(total_loss)
#         self.recon_loss_tracker.update_state(recon_loss)
#         self.vq_loss_tracker.update_state(sum(self.vqvae.losses))

#         return {
#             "loss": self.total_loss_tracker.result(),
#             "reconstruction_loss": self.recon_loss_tracker.result(),
#             "vq_loss": self.vq_loss_tracker.result(),
#         }

# def get_vqvae(latent_dim=256, num_embeddings=1024):
#     encoder = get_encoder(latent_dim)
#     decoder = get_decoder(latent_dim)
#     vq_layer = VectorQuantizer(num_embeddings, latent_dim)
    
#     inputs = keras.Input(shape=(224, 224, 3))
#     latents = encoder(inputs)
#     quantized = vq_layer(latents)
#     outputs = decoder(quantized)
    
#     return keras.Model(inputs, outputs, name="vq_vae")

# # ----------------- VQ-VAE Trainer -----------------


# # ----------------- Example Usage -----------------
# if __name__ == "__main__":
#     # Example: create trainer and see summary
#     trainer = VQVAETrainer(latent_dim=128, num_embeddings=512)
#     trainer.vqvae.summary()



import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# ----------------- Vector Quantizer -----------------
# class VectorQuantizer(layers.Layer):
#     def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):
#         super().__init__(**kwargs)
#         self.embedding_dim = embedding_dim
#         self.num_embeddings = num_embeddings
#         self.beta = beta

#         initializer = tf.random_uniform_initializer()
#         self.embeddings = self.add_weight(
#             shape=(num_embeddings, embedding_dim),
#             initializer=initializer,
#             trainable=True,
#             name="embeddings"
#         )

#     def call(self, x, return_indices=False):
#         """
#         x: (batch, H, W, D)
#         return_indices: whether to return the discrete token indices
#         """
#         input_shape = tf.shape(x)
#         flattened = tf.reshape(x, [-1, self.embedding_dim])

#         # compute distances to embedding
#         distances = (
#             tf.reduce_sum(flattened**2, axis=1, keepdims=True)
#             - 2 * tf.matmul(flattened, self.embeddings, transpose_b=True)
#             + tf.reduce_sum(self.embeddings**2, axis=1)
#         )

#         encoding_indices = tf.argmin(distances, axis=1)  # tokens
#         encodings = tf.one_hot(encoding_indices, self.num_embeddings)
#         quantized = tf.matmul(encodings, self.embeddings)
#         quantized = tf.reshape(quantized, input_shape)

#         # losses
#         codebook_loss = tf.reduce_mean((tf.stop_gradient(x) - quantized) ** 2)
#         commitment_loss = tf.reduce_mean((x - tf.stop_gradient(quantized)) ** 2)
#         self.add_loss(codebook_loss + self.beta * commitment_loss)

#         # straight-through estimator
#         quantized = x + tf.stop_gradient(quantized - x)

#         if return_indices:
#             tokens = tf.reshape(encoding_indices, input_shape[:-1])
#             return quantized, tokens
#         return quantized

vgg = tf.keras.applications.VGG19(include_top=False, weights="imagenet")
vgg = keras.Model(vgg.input, vgg.get_layer("block1_conv1").output)
vgg.trainable = False
def perceptual_loss(x, y):
    x_v = tf.keras.applications.vgg19.preprocess_input(x * 255.0)
    y_v = tf.keras.applications.vgg19.preprocess_input(y * 255.0)
    fx = vgg(x_v) / 255.0
    fy = vgg(y_v) / 255.0
    return tf.reduce_mean(tf.square(fx - fy))
class VectorQuantizerEMA(layers.Layer):
    def __init__(self, num_embeddings, embedding_dim, decay=0.99, epsilon=1e-5, **kwargs):
        super().__init__(**kwargs)
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.decay = decay
        self.epsilon = epsilon
        
        initializer = tf.random_uniform_initializer()
        self.embeddings = self.add_weight(
            shape=(num_embeddings, embedding_dim),
            initializer=initializer,
            trainable=False,
            name="embeddings"
        )
        self.ema_cluster_size = self.add_weight(
            shape=(num_embeddings,),
            initializer=tf.zeros_initializer(),
            trainable=False
        )
        self.ema_embeddings = self.add_weight(
            shape=(num_embeddings, embedding_dim),
            initializer=initializer,
            trainable=False
        )

    def call(self, x, return_indices=False):
        flat_x = tf.reshape(x, [-1, self.embedding_dim])

        # distances
        distances = (
            tf.reduce_sum(flat_x**2, 1, keepdims=True)
            - 2 * tf.matmul(flat_x, self.embeddings, transpose_b=True)
            + tf.reduce_sum(self.embeddings**2, axis=1)
        )
        encoding_indices = tf.argmin(distances, axis=1)
        encodings = tf.one_hot(encoding_indices, self.num_embeddings)

        # quantize
        quantized = tf.matmul(encodings, self.embeddings)
        quantized = tf.reshape(quantized, tf.shape(x))

        # EMA updates
        cluster_size = tf.reduce_sum(encodings, axis=0)
        embedding_sum = tf.matmul(encodings, flat_x, transpose_a=True)

        self.ema_cluster_size.assign(
            self.decay * self.ema_cluster_size + (1 - self.decay) * cluster_size
        )
        self.ema_embeddings.assign(
            self.decay * self.ema_embeddings + (1 - self.decay) * embedding_sum
        )

        n = tf.reduce_sum(self.ema_cluster_size)
        cluster_size = (
            (self.ema_cluster_size + self.epsilon)
            / (n + self.num_embeddings * self.epsilon) * n
        )

        self.embeddings.assign(self.ema_embeddings / tf.reshape(cluster_size, [-1, 1]))

        # straight-through
        quantized = x + tf.stop_gradient(quantized - x)

        if return_indices:
            tokens = tf.reshape(encoding_indices, tf.shape(x)[:-1])
            return quantized, tokens
        return quantized

# ----------------- Encoder -----------------
# def get_encoder(latent_dim=256):
#     inputs = keras.Input(shape=(224, 224, 3))
#     x = layers.Conv2D(128, 4, strides=2, padding="same", activation="relu")(inputs)  # 224->112
#     x = layers.Conv2D(256, 4, strides=2, padding="same", activation="relu")(x)       # 112->56
#     x = layers.Conv2D(512, 4, strides=2, padding="same", activation="relu")(x)       # 56->28
#     x = layers.Conv2D(latent_dim, 1, padding="same")(x)                               # 28x28xlatent_dim
#     return keras.Model(inputs, x, name="encoder")
def get_encoder(latent_dim=512):
    inputs = keras.Input(shape=(224, 224, 3))
    x = layers.Conv2D(128, 4, strides=2, padding="same", activation="relu")(inputs)   # 224 -> 112
    x = layers.Conv2D(256, 4, strides=2, padding="same", activation="relu")(x)        # 112 -> 56
    
    # No third downsample → MUCH less blur
    x = layers.Conv2D(latent_dim, 1, padding="same")(x)                                # 56×56×latent_dim
    
    return keras.Model(inputs, x, name="encoder")

# ----------------- Decoder -----------------
# def get_decoder(latent_dim=256):
#     inputs = keras.Input(shape=(28, 28, latent_dim))
#     x = layers.Conv2DTranspose(512, 4, strides=2, padding="same", activation="relu")(inputs)  # 28->56
#     x = layers.Conv2DTranspose(256, 4, strides=2, padding="same", activation="relu")(x)       # 56->112
#     x = layers.Conv2DTranspose(128, 4, strides=2, padding="same", activation="relu")(x)       # 112->224
#     outputs = layers.Conv2DTranspose(3, 3, padding="same", activation="sigmoid")(x)           # final output
#     return keras.Model(inputs, outputs, name="decoder")

def get_decoder(latent_dim=512):
    inputs = keras.Input(shape=(56, 56, latent_dim))
    x = layers.Conv2DTranspose(256, 4, strides=2, padding="same", activation="relu")(inputs)   # 56 -> 112
    x = layers.Conv2DTranspose(128, 4, strides=2, padding="same", activation="relu")(x)        # 112 -> 224
    outputs = layers.Conv2DTranspose(3, 3, padding="same", activation="sigmoid")(x)
    return keras.Model(inputs, outputs, name="decoder")


# ----------------- VQ-VAE Model -----------------
def get_vqvae(latent_dim=256, num_embeddings=1024):
    encoder = get_encoder(latent_dim)
    decoder = get_decoder(latent_dim)
    vq_layer = VectorQuantizer(num_embeddings, latent_dim)
    
    inputs = keras.Input(shape=(224, 224, 3))
    latents = encoder(inputs)
    quantized, tokens = vq_layer(latents, return_indices=True)
    outputs = decoder(quantized)
    
    return keras.Model(inputs, [outputs, tokens], name="vq_vae_with_tokens"), encoder, decoder, vq_layer

# ----------------- Decode from Tokens -----------------
def decode_from_tokens(tokens, vq_layer, decoder):
    """
    Reconstruct images purely from token indices.
    tokens: (batch, H, W) integer indices
    vq_layer: trained VectorQuantizer
    decoder: trained decoder model
    """
    batch_size, H, W = tokens.shape
    flat_tokens = tf.reshape(tokens, [-1])
    embeddings = tf.gather(vq_layer.embeddings, flat_tokens)
    quantized_latents = tf.reshape(embeddings, (batch_size, H, W, vq_layer.embedding_dim))
    reconstructed = decoder(quantized_latents)
    return reconstructed

# ----------------- VQVAE Trainer -----------------
class VQVAETrainer(tf.keras.Model):
    def __init__(self, latent_dim=256, num_embeddings=1024, **kwargs):
        super().__init__(**kwargs)
        self.vqvae, self.encoder, self.decoder, self.vq_layer = get_vqvae(latent_dim, num_embeddings)
        self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
        self.recon_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
        self.vq_loss_tracker = tf.keras.metrics.Mean(name="vq_loss")

    @property
    def metrics(self):
        return [self.total_loss_tracker, self.recon_loss_tracker, self.vq_loss_tracker]

    def train_step(self, x):
        with tf.GradientTape() as tape:
            recon, _ = self.vqvae(x)
            recon_loss = tf.reduce_mean((x - recon) ** 2)
            total_loss = recon_loss + sum(self.vqvae.losses)

        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))

        self.total_loss_tracker.update_state(total_loss)
        self.recon_loss_tracker.update_state(recon_loss)
        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))

        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.recon_loss_tracker.result(),
            "vq_loss": self.vq_loss_tracker.result(),
        }
# class VQVAETrainer(tf.keras.Model):
#     def __init__(self, latent_dim=512, num_embeddings=1024, **kwargs):
#         super().__init__(**kwargs)
#         self.vqvae, self.encoder, self.decoder, self.vq_layer = get_vqvae(latent_dim, num_embeddings)
#         self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
#         self.recon_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
#         self.perc_loss_tracker = tf.keras.metrics.Mean(name="perceptual_loss")
#         self.vq_loss_tracker = tf.keras.metrics.Mean(name="vq_loss")

#     @property
#     def metrics(self):
#         return [
#             self.total_loss_tracker,
#             self.recon_loss_tracker,
#             self.perc_loss_tracker,
#             self.vq_loss_tracker,
#         ]

#     def train_step(self, x):
#         with tf.GradientTape() as tape:
#             recon, _ = self.vqvae(x)

#             # Pixel MSE
#             mse = tf.reduce_mean((x - recon) ** 2)

#             # Perceptual (VGG)
#             perc = perceptual_loss(x, recon)

#             # Combined reconstruction loss
#             recon_loss = mse * 0.8 + perc * 0.001

#             # Vector quantization loss
#             vq_loss = sum(self.vqvae.losses)

#             total_loss = recon_loss + vq_loss

#         grads = tape.gradient(total_loss, self.vqvae.trainable_variables)
#         self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))

#         self.total_loss_tracker.update_state(total_loss)
#         self.recon_loss_tracker.update_state(mse)
#         self.perc_loss_tracker.update_state(perc)
#         self.vq_loss_tracker.update_state(vq_loss)

#         return {
#             "loss": self.total_loss_tracker.result(),
#             "mse_loss": self.recon_loss_tracker.result(),
#             "perceptual_loss": self.perc_loss_tracker.result(),
#             "vq_loss": self.vq_loss_tracker.result(),
#         }

# ----------------- Example Usage -----------------
if __name__ == "__main__":
    import matplotlib.pyplot as plt

    def show_subplot(original, reconstructed):
        plt.figure(figsize=(6, 3))
        plt.subplot(1, 2, 1)
        plt.imshow(np.clip(original, 0, 1))
        plt.title("Original")
        plt.axis("off")
        plt.subplot(1, 2, 2)
        plt.imshow(np.clip(reconstructed, 0, 1))
        plt.title("Reconstructed")
        plt.axis("off")
        plt.show()

    # Initialize trainer
    trainer = VQVAETrainer(latent_dim=256, num_embeddings=1024)
    trainer.compile(optimizer=tf.keras.optimizers.Adam())

    # Dummy images
    x_dummy = np.random.rand(2, 224, 224, 3).astype(np.float32)

#     # Forward pass: get reconstruction + tokens



import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image

IMAGE_DIR = "/kaggle/input/flickr30k-dataset/flickr30k/Images"
IMG_SIZE = (224, 224)
MAX_IMAGES = 400

def load_first_n_images(folder, img_size, n):
    imgs = []
    files = sorted(os.listdir(folder))[:n]
    for fname in files:
        img = image.load_img(os.path.join(folder, fname), target_size=img_size)
        img = image.img_to_array(img)
        imgs.append(img)
    return np.array(imgs, dtype=np.float32)

all_images = load_first_n_images(IMAGE_DIR, IMG_SIZE, MAX_IMAGES)
print("Loaded:", all_images.shape)

# Normalize 0–1
all_images = all_images / 255.0

x_train = all_images[:360]
x_test = all_images[360:]

print("Train variance:", np.var(x_train))

# --- Use improved VQ-VAE from previous step -
latent_dim = 128
num_embeddings = 512

vqvae_trainer = VQVAETrainer(latent_dim=latent_dim, num_embeddings=num_embeddings)
vqvae_trainer.compile(optimizer=tf.keras.optimizers.Adam(1e-3))

# Reduce batch size for larger images
vqvae_trainer.fit(x_train, epochs=50, batch_size=16)




import matplotlib.pyplot as plt
import numpy as np

def show_subplot(original, reconstructed):
    plt.figure(figsize=(6, 3))
    plt.subplot(1, 2, 1)
    plt.imshow(np.clip(original, 0, 1))
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(np.clip(reconstructed, 0, 1))
    plt.title("Reconstructed")
    plt.axis("off")
    plt.show()


# Sample random test images
idx = np.random.choice(len(x_train), 10, replace=False)
test_images = x_train[idx]

print(test_images.shape)

# Use the trained VQ-VAE model for reconstruction
# Replace with your actual trained VQ-VAE model
vqvae_model = vqvae_trainer.vqvae  

# Reconstruct images
reconstructions_test = vqvae_model.predict(test_images, batch_size=4)[0]
# print(reconstructions_test)
print(type(reconstructions_test))  # likely list
print(len(reconstructions_test))   # maybe 2 (reconstruction + vq loss)
# Display images one by one
for original, reconstructed in zip(test_images, reconstructions_test):
    # Squeeze in case of extra batch dimension
    print(reconstructed.shape)
    print(original.shape)
    show_subplot(np.squeeze(original), np.squeeze(reconstructed))





import matplotlib.pyplot as plt
import numpy as np

def show_subplot(original, reconstructed):
    plt.figure(figsize=(6, 3))
    plt.subplot(1, 2, 1)
    plt.imshow(np.clip(original, 0, 1))
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(np.clip(reconstructed, 0, 1))
    plt.title("Reconstructed")
    plt.axis("off")
    plt.show()


# Sample random test images
idx = np.random.choice(len(x_train), 10, replace=False)
test_images = x_train[idx]

# vqvae_model = vqvae_trainer.vqvae  
# reconstructions_test = vqvae_model.predict(test_images, batch_size=4)[0]
# tokens = vqvae_model.predict(test_images, batch_size=4)[1]
# print(type(reconstructions_test))  # likely list
# print(tokens)   # maybe 2 (reconstruction + vq loss)
# # Display images one by one
# for original, reconstructed in zip(test_images, reconstructions_test):
#     # Squeeze in case of extra batch dimension
#     print(reconstructed.shape)
#     print(original.shape)
#     show_subplot(np.squeeze(original), np.squeeze(reconstructed))


#------------------------------image-genration-using -tokens--------------

# Get the trained VQVAE model
vqvae_model = vqvae_trainer.vqvae

# Get reconstructions AND tokens from the model
recons, tokens = vqvae_model.predict(test_images, batch_size=4)

print("Recon shape:", recons.shape)
print("Token grid shape:", tokens.shape)    # (batch, H, W)

# Now reconstruct only from tokens
decoded_from_tokens = decode_from_tokens(tokens, 
                                         vqvae_trainer.vq_layer, 
                                         vqvae_trainer.decoder)

print("Decoded-from-tokens shape:", decoded_from_tokens.shape)

# Display reconstructions
for i in range(len(test_images)):
    original = test_images[i]
    rec_from_tokens = decoded_from_tokens[i].numpy()

    print("Original:", original.shape)
    print("From tokens:", rec_from_tokens.shape)

    show_subplot(original, rec_from_tokens)
