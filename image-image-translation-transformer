import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from einops import rearrange

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),  # to [-1,1]
])


class ImageToImageDataset(Dataset):
    def __init__(self, root_dir, transform=None, patch_size=4):
        self.rootA = os.path.join(root_dir, "trainA")
        self.rootB = os.path.join(root_dir, "trainB")
        self.transform = transform
        self.patch_size = patch_size
        
        self.filesA = sorted(os.listdir(self.rootA))[:1067]
        self.filesB = sorted(os.listdir(self.rootB))[:1067]  # Match length

    def __len__(self):
        return len(self.filesA)

    def _to_patches(self, img):
        # img shape: (3, H, W) e.g., (3, 256, 256)
        p = self.patch_size
        patches = rearrange(
            img, 
            'c (h p1) (w p2) -> (h w) (c p1 p2)', 
            p1=p, p2=p
        )
        return patches  # shape: (num_patches, patch_dim)

    def __getitem__(self, idx):
        imgA_path = os.path.join(self.rootA, self.filesA[idx])
        imgB_path = os.path.join(self.rootB, self.filesB[idx])

        imgA = Image.open(imgA_path).convert("RGB")
        imgB = Image.open(imgB_path).convert("RGB")

        if self.transform:
            imgA = self.transform(imgA)  # shape (3,256,256)
            imgB = self.transform(imgB)

        # STEP 2: Convert images → patches
        patchesA = self._to_patches(imgA)
        # patchesB = self._to_patches(imgB)

        return patchesA, imgB


dataset_path = "/kaggle/input/horse2zebra-dataset"

dataset = ImageToImageDataset(dataset_path, transform)

dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

A, B = next(iter(dataloader))
print("A batch:", A.shape)   # (batch, 3, 256, 256)
print("B batch:", B.shape)   # (batch, 3, 256, 256)




import torch.nn as nn

class PatchEmbedding(nn.Module):
    def __init__(self, patch_dim=768, embed_dim=256):
        super().__init__()
        self.linear = nn.Linear(patch_dim, embed_dim)

    def forward(self, patches):
        # patches: (batch, num_patches, patch_dim)
        embeddings = self.linear(patches)
        return embeddings  # shape: (batch, num_patches, embed_dim)
        
embedder = PatchEmbedding(patch_dim=48, embed_dim=256)

A, B = next(iter(dataloader))  # (4, 256, 768)

A_embed = embedder(A)  # (4, 256, 256)
# B_embed = embedder(B)  # (4, 256, 256)

print(A_embed.shape)



# import torch
# class PositionalEncoding(nn.Module):
#     def __init__(self, num_patches=256, embed_dim=256):
#         super().__init__()
#         self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))

#     def forward(self, x):
#         # x: (batch, numpatches, embed_dim)
#         return x + self.pos_embed

# pos_encoder = PositionalEncoding(num_patches=256, embed_dim=256)
# A_out = pos_encoder(A_embed)   # (4, 256, 256)
# # B_out = pos_encoder(B_embed)   # (4, 256, 256)

# print(A_out.shape)

import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim=256, grid_size=16):
        """
        grid_size = H = W = sqrt(num_patches)
        embed_dim must be divisible by 4 for 2D split
        """
        super().__init__()
        self.embed_dim = embed_dim
        
        # separate row and column embeddings
        self.row_embed = nn.Parameter(torch.zeros(1, grid_size, embed_dim // 2))
        self.col_embed = nn.Parameter(torch.zeros(1, grid_size, embed_dim // 2))
        
        # Xavier init = MUCH more stable
        nn.init.xavier_uniform_(self.row_embed)
        nn.init.xavier_uniform_(self.col_embed)

    def forward(self, x):
        B, N, D = x.shape
        H = W = int(math.sqrt(N))

        # reshape (B, H, W, embed)
        x = x.view(B, H, W, D)

        # row_embed: (1, H, D/2) → expand to (B, H, W, D/2)
        # col_embed: (1, W, D/2) → expand to (B, H, W, D/2)
        pos = torch.cat([
            self.row_embed[:, :, None, :].expand(B, -1, W, -1),
            self.col_embed[:, None, :, :].expand(B, H, -1, -1)
        ], dim=3)

        # final shape → (B, N, D)
        x = x + pos
        return x.view(B, N, D)

pos_encoder = PositionalEncoding(num_patches=4096, embed_dim=256)
A_out = pos_encoder(A_embed)   # (4, 256, 256)
# B_out = pos_encoder(B_embed)   # (4, 256, 256)

print(A_out.shape)


class TransformerEncoderBlock(nn.Module):
    def __init__(self, embed_dim=256, num_heads=8, mlp_ratio=4.0):
        super().__init__()

        # LayerNorm before attention (ViT style)
        self.norm1 = nn.LayerNorm(embed_dim)

        # Multi-head self-attention
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)

        # Feed Forward Network
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),
            nn.GELU(),
            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)
        )

    def forward(self, x):
        # x: (batch, num_patches, embed_dim)

        # --- Self Attention ---
        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))
        x = x + attn_out  # residual

        # --- MLP ---
        mlp_out = self.mlp(self.norm2(x))
        x = x + mlp_out  # residual

        return x
encoder = TransformerEncoderBlock(embed_dim=256, num_heads=8)

A_encoded = encoder(A_out)      # Step 5   (batch,256,256)
# B_encoded = encoder(B_out)

print(A_encoded.shape)



class PatchDecoder(nn.Module):
    def __init__(self, embed_dim=256, patch_size=4, out_channels=3):
        super().__init__()
        self.patch_size = patch_size
        self.out_channels = out_channels

        # Invert patch embedding: 256 → 768
        self.linear = nn.Linear(embed_dim, out_channels * patch_size * patch_size)

    def forward(self, x):
        # x: (batch, num_patches, embed_dim)
        B, N, D = x.shape
        p = self.patch_size

        # 1) Project back to pixels
        x = self.linear(x)                 # (B, N, 768)

        # 2) Reshape to (B, N, 3, 16,16)
        x = x.view(B, N, self.out_channels, p, p)

        # 3) Reassemble patches into image
        H = W = int((N)**0.5)              # 16×16 patches → 256 total

        x = x.view(B, H, W, self.out_channels, p, p)
        x = x.permute(0,3,1,4,2,5)         # (B,3,H,p,W,p)
        x = x.reshape(B, self.out_channels, H*p, W*p)  # (B,3,256,256)

        return x

decoder = PatchDecoder(embed_dim=256, patch_size=4)
# A_enc   = encoder(A_out)       # Step 5

A_rec   = decoder(A_encoded)       # Step 6 → reconstructed image

print(A_rec.shape)


# -------------------------
# STEP 7 — TRAINING LOOP
# -------------------------

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.utils import save_image
import os

device = "cuda" if torch.cuda.is_available() else "cpu"

# Move models to device
embedder = embedder.to(device)
pos_encoder = pos_encoder.to(device)
encoder = encoder.to(device)
decoder = decoder.to(device)

# Loss function
criterion = nn.L1Loss()   # L1 works best for image reconstruction
# criterion = nn.MSELoss()  # optional

# Optimizer (all model params)
optimizer = optim.AdamW(
    list(embedder.parameters()) + 
    list(pos_encoder.parameters()) + 
    list(encoder.parameters()) +
    list(decoder.parameters()),
    lr=1e-4,
    weight_decay=1e-4
)

# Create output folder
os.makedirs("training_outputs", exist_ok=True)

EPOCHS = 10

for epoch in range(EPOCHS):
    total_loss = 0
    for A, B in dataloader:

        # Move data to GPU
        A = A.to(device)            # (B, 256, 768)
        B = B.to(device)            # (B, 3, 256, 256)

        # ------------------------------
        # Forward Pass (Steps 3→6)
        # ------------------------------
        A_embed = embedder(A)         # Step 3
        A_pos   = pos_encoder(A_embed) # Step 4
        A_enc   = encoder(A_pos)       # Step 5
        A_rec   = decoder(A_enc)       # Step 6 (output image)

        # ------------------------------
        # Compute loss
        # ------------------------------
        loss = criterion(A_rec, B)
        total_loss += loss.item()

        # ------------------------------
        # Backprop
        # ------------------------------
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(dataloader)
    print(f"Epoch [{epoch+1}/{EPOCHS}]  Loss: {avg_loss:.4f}")

    # --------------------------------
    # Save sample reconstructions each epoch
    # --------------------------------
    save_image(A_rec[:4], f"training_outputs/recon_epoch_{epoch+1}.png")
    save_image(B[:4],     f"training_outputs/target_epoch_{epoch+1}.png")



import matplotlib.pyplot as plt
import torch
from torchvision import transforms
from PIL import Image
from einops import rearrange

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load demo image
demo_path = "/kaggle/input/horse2zebra-dataset/testA/n02381460_1000.jpg"
img = Image.open(demo_path).convert("RGB")

# Preprocessing
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),
])
img_tensor = transform(img)  # (3, 256, 256)

# Patchify
patch_size = 4
patches = rearrange(
    img_tensor, 
    'c (h p1) (w p2) -> 1 (h w) (c p1 p2)',
    p1=patch_size, p2=patch_size
).to(device)  # (1, 256, 768)

# Forward pass through trained model
with torch.no_grad():
    A_embed = embedder(patches)
    A_pos   = pos_encoder(A_embed)
    A_enc   = encoder(A_pos)
    A_rec   = decoder(A_enc)  # (1, 3, 256, 256)

# Denormalize for visualization
def denormalize(t):
    return t * 0.5 + 0.5  # [-1,1] -> [0,1]

input_img = denormalize(img_tensor).permute(1,2,0).cpu().numpy()
output_img = denormalize(A_rec[0]).permute(1,2,0).cpu().numpy()

# Plot side by side
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.title("Input Image")
plt.imshow(input_img)
plt.axis("off")

plt.subplot(1,2,2)
plt.title("Reconstructed Image")
plt.imshow(output_img)
plt.axis("off")

plt.show()

