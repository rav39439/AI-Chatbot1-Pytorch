import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load CSV
df = pd.read_csv('/kaggle/input/testdata/sampled_20_percent.csv')

# Drop duplicates and NaNs
df = df.drop_duplicates().dropna()
df=df.head(30000)
print(df.head(20))

# Encode categorical features
cat_features = ['category_id', 'category_code', 'brand', 'product_id']
label_encoders = {}
for col in cat_features:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Normalize price
scaler = MinMaxScaler()
df['price'] = scaler.fit_transform(df[['price']])


# Sort by user and time (if you have timestamp)
# For now assuming dataframe is in purchase order
user_sequences = df.groupby('user_id').apply(
    lambda x: x[['product_id', 'category_id', 'category_code', 'brand', 'price']].values.tolist()
).to_dict()



MAX_SEQ_LEN = 10  # maximum sequence length

X = []
y = []

for user, seq in user_sequences.items():
    for i in range(1, len(seq)):
        input_seq = seq[:i]  # all previous items
        target = seq[i][0]   # predict next product_id
        X.append(input_seq)
        y.append(target)

# Pad sequences (for multiple features, we need 3D array)
def pad_multi_features(sequences, maxlen):
    padded = []
    for seq in sequences:
        # seq is list of [product_id, category_id, category_code, brand, price]
        seq = seq[-maxlen:]  # keep last maxlen
        pad_len = maxlen - len(seq)
        if pad_len > 0:
            seq = [[0]*5]*pad_len + seq  # pad with zeros
        padded.append(seq)
    return np.array(padded)

X_pad = pad_multi_features(X, MAX_SEQ_LEN)
y = np.array(y)

print(X_pad)

# Step 4: Build GRU Model
# Using all features per product.
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Masking, Embedding, Input, Flatten

# Split features
product_ids = X_pad[:,:,0]
category_ids = X_pad[:,:,1]
category_codes = X_pad[:,:,2]
brands = X_pad[:,:,3]
prices = X_pad[:,:,4]

VOCAB_SIZE_PRODUCT = df['product_id'].nunique()
VOCAB_SIZE_CATEGORY = df['category_id'].nunique()
VOCAB_SIZE_CATEGORY_CODE = df['category_code'].nunique()
VOCAB_SIZE_BRAND = df['brand'].nunique()

EMB_SIZE = 32

# Input layers
input_product = Input(shape=(MAX_SEQ_LEN,))
input_category = Input(shape=(MAX_SEQ_LEN,))
input_catcode = Input(shape=(MAX_SEQ_LEN,))
input_brand = Input(shape=(MAX_SEQ_LEN,))
input_price = Input(shape=(MAX_SEQ_LEN,1))

# Embeddings
embed_product = Embedding(VOCAB_SIZE_PRODUCT+1, EMB_SIZE)(input_product)
embed_category = Embedding(VOCAB_SIZE_CATEGORY+1, EMB_SIZE)(input_category)
embed_catcode = Embedding(VOCAB_SIZE_CATEGORY_CODE+1, EMB_SIZE)(input_catcode)
embed_brand = Embedding(VOCAB_SIZE_BRAND+1, EMB_SIZE)(input_brand)

# Concatenate embeddings + price
x = tf.keras.layers.Concatenate()([embed_product, embed_category, embed_catcode, embed_brand, input_price])

# GRU layer
x = GRU(128)(x)

# Output layer (predict next product)
output = Dense(VOCAB_SIZE_PRODUCT, activation='softmax')(x)

model = tf.keras.Model(inputs=[input_product, input_category, input_catcode, input_brand, input_price],
                       outputs=output)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()


# Step 5: Train the Model
# Reshape prices to 3D
input_price = prices.reshape(prices.shape[0], prices.shape[1], 1)

model.fit([product_ids, category_ids, category_codes, brands, input_price],
          y,
          batch_size=64,
          epochs=90,
          validation_split=0.1)


def recommend_next_products(model, user_seq, top_k=5):
    # Pad user sequence
    seq_padded = pad_multi_features([user_seq], MAX_SEQ_LEN)
    product_ids = seq_padded[:,:,0]
    category_ids = seq_padded[:,:,1]
    category_codes = seq_padded[:,:,2]
    brands = seq_padded[:,:,3]
    prices = seq_padded[:,:,4].reshape(1, MAX_SEQ_LEN, 1)
    
    preds = model.predict([product_ids, category_ids, category_codes, brands, prices])[0]
    top_products_idx = preds.argsort()[-top_k:][::-1]
    
    # Convert back to original product IDs
    le = label_encoders['product_id']
    return le.inverse_transform(top_products_idx)


#------------------------------lstm in place of GRU------------------------------
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Embedding, Input

# Split features
product_ids = X_pad[:,:,0]
category_ids = X_pad[:,:,1]
category_codes = X_pad[:,:,2]
brands = X_pad[:,:,3]
prices = X_pad[:,:,4]

VOCAB_SIZE_PRODUCT = df['product_id'].nunique()
VOCAB_SIZE_CATEGORY = df['category_id'].nunique()
VOCAB_SIZE_CATEGORY_CODE = df['category_code'].nunique()
VOCAB_SIZE_BRAND = df['brand'].nunique()

EMB_SIZE = 32

# Inputs
input_product = Input(shape=(MAX_SEQ_LEN,))
input_category = Input(shape=(MAX_SEQ_LEN,))
input_catcode = Input(shape=(MAX_SEQ_LEN,))
input_brand = Input(shape=(MAX_SEQ_LEN,))
input_price = Input(shape=(MAX_SEQ_LEN,1))

# Embeddings
embed_product = Embedding(VOCAB_SIZE_PRODUCT+1, EMB_SIZE)(input_product)
embed_category = Embedding(VOCAB_SIZE_CATEGORY+1, EMB_SIZE)(input_category)
embed_catcode = Embedding(VOCAB_SIZE_CATEGORY_CODE+1, EMB_SIZE)(input_catcode)
embed_brand = Embedding(VOCAB_SIZE_BRAND+1, EMB_SIZE)(input_brand)

# Concatenate all features
x = tf.keras.layers.Concatenate()([
    embed_product,
    embed_category,
    embed_catcode,
    embed_brand,
    input_price
])

# ðŸ”¥ LSTM layer (replacing GRU)
x = LSTM(128, return_sequences=False)(x)

# Output layer
output = Dense(VOCAB_SIZE_PRODUCT, activation='softmax')(x)

model = Model(
    inputs=[input_product, input_category, input_catcode, input_brand, input_price],
    outputs=output
)

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
# Step 5: Train the Model
# Reshape prices to 3D
input_price = prices.reshape(prices.shape[0], prices.shape[1], 1)

model.fit([product_ids, category_ids, category_codes, brands, input_price],
          y,
          batch_size=64,
          epochs=90,
          validation_split=0.1)


def recommend_next_products(model, user_seq, top_k=5):
    # Pad user sequence
    seq_padded = pad_multi_features([user_seq], MAX_SEQ_LEN)
    product_ids = seq_padded[:,:,0]
    category_ids = seq_padded[:,:,1]
    category_codes = seq_padded[:,:,2]
    brands = seq_padded[:,:,3]
    prices = seq_padded[:,:,4].reshape(1, MAX_SEQ_LEN, 1)
    
    preds = model.predict([product_ids, category_ids, category_codes, brands, prices])[0]
    top_products_idx = preds.argsort()[-top_k:][::-1]
    
    # Convert back to original product IDs
    le = label_encoders['product_id']
    return le.inverse_transform(top_products_idx)


# Example
user_example = user_sequences[list(user_sequences.keys())[0]]
recommend_next_products(model, user_example)


#---------------------------------------------------------------------------------

# Example
user_example = user_sequences[list(user_sequences.keys())[0]]
recommend_next_products(model, user_example)

