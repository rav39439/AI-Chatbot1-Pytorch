# ============================================================
# STEP 1: UNDERSTAND THE PROBLEM
# ------------------------------------------------------------
# Task: Multi-class image classification
# Input: Images in images/ folder
# Output: Object class label
# Metric: Accuracy
# ============================================================


# ============================================================
# STEP 2: COLLECT AND LOAD THE DATASET
# ============================================================

-------------------------------------------------ANN -image-classification-------------------------------------
import os
import numpy as np
from PIL import Image

IMAGE_DIR = "images"
IMAGE_SIZE = (64, 64)

X = []  # image data
y = []  # labels

class_names = sorted(os.listdir(IMAGE_DIR))
num_classes = len(class_names)

print("Classes:", class_names)

for label, class_name in enumerate(class_names):
    class_path = os.path.join(IMAGE_DIR, class_name)

    for img_file in os.listdir(class_path):
        img_path = os.path.join(class_path, img_file)

        try:
            img = Image.open(img_path).convert("RGB")
            img = img.resize(IMAGE_SIZE)
            img_array = np.array(img)

            X.append(img_array)
            y.append(label)

        except Exception as e:
            print(f"Skipping {img_path}: {e}")

X = np.array(X)
y = np.array(y)

print("Raw image shape:", X.shape)
print("Labels shape:", y.shape)


# ============================================================
# STEP 3: DATA PREPROCESSING
# ============================================================

# Normalize pixel values
X = X.astype("float32") / 255.0

# Flatten images for ANN
X = X.reshape(X.shape[0], -1)
print("Flattened input shape:", X.shape)

# One-hot encode labels
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)
y_encoded = encoder.fit_transform(y.reshape(-1, 1))

print("Encoded labels shape:", y_encoded.shape)


# ============================================================
# STEP 4: SPLIT THE DATASET
# ============================================================

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42
)

print("Train samples:", X_train.shape[0])
print("Test samples:", X_test.shape[0])


# ============================================================
# STEP 5: DESIGN THE ANN ARCHITECTURE
# ============================================================

input_size = X_train.shape[1]
num_classes = y_train.shape[1]

print("Input size:", input_size)
print("Number of classes:", num_classes)


# ============================================================
# STEP 6: INITIALIZE MODEL PARAMETERS (PyTorch)
# ============================================================

import torch
import torch.nn as nn

X_train_t = torch.tensor(X_train, dtype=torch.float32)
X_test_t = torch.tensor(X_test, dtype=torch.float32)

y_train_t = torch.tensor(y_train, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.float32)

y_train_idx = torch.argmax(y_train_t, dim=1)
y_test_idx = torch.argmax(y_test_t, dim=1)


class ANN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(ANN, self).__init__()

        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)

        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)   # No softmax here
        return x


model = ANN(input_size, num_classes)


# ============================================================
# STEP 7: CHOOSE LOSS FUNCTION
# ============================================================

criterion = nn.CrossEntropyLoss()


# ============================================================
# STEP 8: CHOOSE OPTIMIZER
# ============================================================

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


# ============================================================
# STEP 9–12: FORWARD PASS, LOSS, BACKPROP, TRAINING
# ============================================================

epochs = 20

for epoch in range(epochs):
    model.train()

    # Forward pass
    outputs = model(X_train_t)

    # Compute loss
    loss = criterion(outputs, y_train_idx)

    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Training accuracy
    with torch.no_grad():
        predictions = torch.argmax(outputs, dim=1)
        accuracy = (predictions == y_train_idx).float().mean()

    print(
        f"Epoch [{epoch+1}/{epochs}] "
        f"Loss: {loss.item():.4f} "
        f"Accuracy: {accuracy.item():.4f}"
    )


# ============================================================
# STEP 13 (OPTIONAL PREVIEW): TEST EVALUATION
# ============================================================

model.eval()
with torch.no_grad():
    test_outputs = model(X_test_t)
    test_predictions = torch.argmax(test_outputs, dim=1)
    test_accuracy = (test_predictions == y_test_idx).float().mean()

print("Test Accuracy:", test_accuracy.item())







-----------------------------------------CNN-version-image-classification-----------------------------------------------

# ============================================================
# STEP 1–2: LOAD IMAGES (Same as before)
# ============================================================

import os
import numpy as np
from PIL import Image

IMAGE_DIR = "images"
IMAGE_SIZE = (64, 64)

X = []
y = []

class_names = sorted(os.listdir(IMAGE_DIR))
num_classes = len(class_names)

print("Classes:", class_names)

for label, class_name in enumerate(class_names):
    class_path = os.path.join(IMAGE_DIR, class_name)

    for img_file in os.listdir(class_path):
        img_path = os.path.join(class_path, img_file)

        try:
            img = Image.open(img_path).convert("RGB")
            img = img.resize(IMAGE_SIZE)
            img_array = np.array(img)

            X.append(img_array)
            y.append(label)

        except Exception as e:
            print(f"Skipping {img_path}: {e}")

X = np.array(X)
y = np.array(y)

print("Raw image shape:", X.shape)


# ============================================================
# STEP 3: PREPROCESSING (CNN VERSION)
# ============================================================

# Normalize
X = X.astype("float32") / 255.0

# Convert to PyTorch format: (N, C, H, W)
X = np.transpose(X, (0, 3, 1, 2))


# ============================================================
# STEP 4: TRAIN / TEST SPLIT
# ============================================================

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# ============================================================
# STEP 5–6: CNN ARCHITECTURE + PARAMETER INIT
# ============================================================

import torch
import torch.nn as nn

X_train_t = torch.tensor(X_train, dtype=torch.float32)
X_test_t = torch.tensor(X_test, dtype=torch.float32)

y_train_t = torch.tensor(y_train, dtype=torch.long)
y_test_t = torch.tensor(y_test, dtype=torch.long)


class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()

        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(64 * 16 * 16, 256)
        self.fc2 = nn.Linear(256, num_classes)

        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.pool(x)

        x = self.relu(self.conv2(x))
        x = self.pool(x)

        x = x.view(x.size(0), -1)

        x = self.relu(self.fc1(x))
        x = self.fc2(x)

        return x


model = CNN(num_classes)


# ============================================================
# STEP 7: LOSS FUNCTION
# ============================================================

criterion = nn.CrossEntropyLoss()


# ============================================================
# STEP 8: OPTIMIZER
# ============================================================

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


# ============================================================
# STEP 9–12: TRAINING LOOP
# ============================================================

epochs = 20

for epoch in range(epochs):
    model.train()

    outputs = model(X_train_t)
    loss = criterion(outputs, y_train_t)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    with torch.no_grad():
        preds = torch.argmax(outputs, dim=1)
        acc = (preds == y_train_t).float().mean()

    print(
        f"Epoch [{epoch+1}/{epochs}] "
        f"Loss: {loss.item():.4f} "
        f"Accuracy: {acc.item():.4f}"
    )


# ============================================================
# STEP 13: TEST EVALUATION
# ============================================================

model.eval()
with torch.no_grad():
    test_outputs = model(X_test_t)
    test_preds = torch.argmax(test_outputs, dim=1)
    test_acc = (test_preds == y_test_t).float().mean()

print("Test Accuracy:", test_acc.item())





-----------------------------------------------text-classification--using- ANN---------------------------------------------------


# ============================================================
# STEP 1–2: Load Dataset
# ============================================================

import pandas as pd

data = pd.read_csv("data.csv")  # columns: 'text', 'label'
texts = data["text"].values
labels = data["label"].values

# ============================================================
# STEP 3: Tokenization (Text → Token IDs)
# ============================================================

from collections import Counter
import re

# Simple tokenizer
def tokenize(text):
    text = text.lower()
    return re.findall(r"\b\w+\b", text)

# Build vocabulary
all_tokens = []
for text in texts:
    all_tokens.extend(tokenize(text))

vocab = Counter(all_tokens)
word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}
word2idx["<PAD>"] = 0
vocab_size = len(word2idx)

# Convert text → sequences + padding
MAX_LEN = 50

def encode_text(text):
    tokens = tokenize(text)
    ids = [word2idx.get(tok, 0) for tok in tokens]
    ids = ids[:MAX_LEN]
    ids += [0] * (MAX_LEN - len(ids))
    return ids

X = [encode_text(text) for text in texts]

# Encode labels
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(labels)
num_classes = len(set(y))

# ============================================================
# STEP 4: Train–Test Split
# ============================================================

from sklearn.model_selection import train_test_split
import torch

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train_t = torch.tensor(X_train, dtype=torch.long)
X_test_t = torch.tensor(X_test, dtype=torch.long)

y_train_t = torch.tensor(y_train, dtype=torch.long)
y_test_t = torch.tensor(y_test, dtype=torch.long)

# ============================================================
# STEP 5–6: ANN + Embedding Architecture
# ============================================================

import torch.nn as nn

class TextEmbeddingANN(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.fc1 = nn.Linear(embed_dim, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        embeds = self.embedding(x)           # [batch_size, seq_len, embed_dim]
        pooled = embeds.mean(dim=1)          # average pooling over sequence
        x = self.relu(self.fc1(pooled))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

EMBED_DIM = 256
model = TextEmbeddingANN(vocab_size=vocab_size, embed_dim=EMBED_DIM, num_classes=num_classes)

# ============================================================
# STEP 7: Loss Function
# ============================================================

criterion = nn.CrossEntropyLoss()

# ============================================================
# STEP 8: Optimizer
# ============================================================

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ============================================================
# STEP 9–12: Training Loop
# ============================================================

EPOCHS = 20

for epoch in range(EPOCHS):
    model.train()
    outputs = model(X_train_t)
    loss = criterion(outputs, y_train_t)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    with torch.no_grad():
        preds = torch.argmax(outputs, dim=1)
        acc = (preds == y_train_t).float().mean()

    print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {loss.item():.4f} Accuracy: {acc.item():.4f}")

# ============================================================
# STEP 13: Evaluation
# ============================================================

model.eval()
with torch.no_grad():
    test_outputs = model(X_test_t)
    test_preds = torch.argmax(test_outputs, dim=1)
    test_acc = (test_preds == y_test_t).float().mean()

print("Test Accuracy:", test_acc.item())




