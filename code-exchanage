import re
from dataclasses import dataclass
from typing import List

from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler
from autogen_core.code_executor import CodeBlock, CodeExecutor
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_ext.models.ollama import OllamaChatCompletionClient
ollamamodel_client = OllamaChatCompletionClient(model="llama3.2")

@dataclass
class Message:
    content: str


@default_subscription
class Assistant(RoutedAgent):
    def __init__(self, model_client: OllamaChatCompletionClient) -> None:
        super().__init__("An assistant agent.")
        self._model_client = ollamamodel_client
        self._chat_history: List[LLMMessage] = [
            SystemMessage(
                content="""Write Python script in markdown block, and it will be executed.
Always save figures to file in the current directory. Do not use plt.show(). All code required to complete this task must be contained within a single response.""",
            )
        ]

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        self._chat_history.append(UserMessage(content=message.content, source="user"))
        result = await self._model_client.create(self._chat_history)
        print(f"\n{'-'*80}\nAssistant:\n{result.content}")
        self._chat_history.append(AssistantMessage(content=result.content, source="assistant"))  # type: ignore
        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore


def extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:
    pattern = re.compile(r"```(?:\s*([\w\+\-]+))?\n([\s\S]*?)```")
    matches = pattern.findall(markdown_text)
    code_blocks: List[CodeBlock] = []
    for match in matches:
        language = match[0].strip() if match[0] else ""
        code_content = match[1]
        code_blocks.append(CodeBlock(code=code_content, language=language))
    return code_blocks


@default_subscription
class Executor(RoutedAgent):
    def __init__(self, code_executor: CodeExecutor) -> None:
        super().__init__("An executor agent.")
        self._code_executor = code_executor

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        code_blocks = extract_markdown_code_blocks(message.content)
        if code_blocks:
            result = await self._code_executor.execute_code_blocks(
                code_blocks, cancellation_token=ctx.cancellation_token
            )
            print(f"\n{'-'*80}\nExecutor:\n{result.output}")
            await self.publish_message(Message(content=result.output), DefaultTopicId())


import tempfile
from dataclasses import dataclass

from autogen_core import SingleThreadedAgentRuntime
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

work_dir = tempfile.mkdtemp()

# Create an local embedded runtime.
runtime = SingleThreadedAgentRuntime()

import asyncio
from autogen_ext.code_executors import DockerCommandLineCodeExecutor
# from autogen_core.models import Message
from autogen_core import DefaultTopicId

# Import your Assistant, Executor, and runtime definitions as needed

async def main():
    work_dir = "./autogen_workspace"

    # Create and enter the async context for DockerCommandLineCodeExecutor
    async with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:
        # Set up your model client
        model_client = ollamamodel_client

        # Register assistant and executor agents
        await Assistant.register(
            runtime,
            "assistant",
            lambda: Assistant(model_client=model_client),
        )
        await Executor.register(runtime, "executor", lambda: Executor(executor))

        # Start the runtime
        runtime.start()

        # Publish a message to the assistant

        @dataclass
        class TextMessage:
            content: str
            source: str

        # Later in your code:
        await runtime.publish_message(
            TextMessage(content="Please analyse …", source="user"),
            DefaultTopicId()
        )

        # Wait for the runtime to finish
        await runtime.stop_when_idle()

        # Close the model client
        await model_client.close()





# from langchain.agents import create_agent
# from langchain.tools import tool
# from langgraph.types import Command
# from langgraph.prebuilt import create_react_agent

# # Define handoff tool from Travel → Hotel
# @tool(
#     name="handoff_to_hotel_advisor",
#     description="Pass control to HotelAdvisorAgent to pick a hotel"
# )
# def handoff_to_hotel(state: dict, tool_call_id: str) -> Command:
#     return Command(
#         goto="hotel_advisor",
#         update={
#             "messages": state["messages"],
#             "destination": state["destination"]
#         }
#     )

# # TravelAdvisorAgent – chooses destination then hands off
# travel_advisor = create_react_agent(
#     model="...",
#     tools=[handoff_to_hotel],
#     prompt="You are a travel advisor. First pick a destination, then you may hand off to HotelAdvisorAgent.",
#     name="travel_advisor"
# )

# # HotelAdvisorAgent – takes destination and recommends hotels
# hotel_advisor = create_react_agent(
#     model="...",
#     tools=[],
#     prompt="You are a hotel advisor. Provide best hotel options in the given destination.",
#     name="hotel_advisor"
# )

# # Build multi‑agent graph
# from langgraph.graph import StateGraph, START, MessagesState
# multi_agent_graph = (
#     StateGraph(MessagesState)
#     .add_node(travel_advisor, destinations=("hotel_advisor",))
#     .add_node(hotel_advisor)
#     .add_edge(START, "travel_advisor")
#     .compile()
# )

# # Run
# for chunk in multi_agent_graph.stream({
#     "messages": [{"role": "user", "content": "I want a holiday in Bali, find flights and a good hotel"}]
# }):
#     print(chunk["messages"][-1]["content"])


# Run the async main function
if __name__ == "__main__":
    asyncio.run(main())
