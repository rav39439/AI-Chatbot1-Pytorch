import os
import torch
import pandas as pd
from torch.utils.data import DataLoader, Dataset
from sentence_transformers import SentenceTransformer
import torch.nn as nn
import torch.nn.functional as F
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

#########################################################
# 1. TEXT ENCODER
#########################################################
class TextEncoder(nn.Module):
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        super().__init__()
        self.model = SentenceTransformer(model_name, device=device)
        self.dim = self.model.get_sentence_embedding_dimension()

    def forward(self, texts):
        # encode already gives a tensor on CPU, so move to device
        emb = self.model.encode(texts, convert_to_tensor=True)
        return emb.to(device)


#########################################################
# 2. VECTOR QUANTIZER
#########################################################
class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings=512, embedding_dim=384, commitment_cost=0.25):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.commitment_cost = commitment_cost
        
        self.embeddings = nn.Embedding(num_embeddings, embedding_dim).to(device)
        nn.init.uniform_(self.embeddings.weight, -1/num_embeddings, 1/num_embeddings)

    def forward(self, z):
        z = z.to(device)

        distances = (
            torch.sum(z ** 2, dim=1, keepdim=True)
            - 2 * torch.matmul(z, self.embeddings.weight.t())
            + torch.sum(self.embeddings.weight**2, dim=1)
        )

        indices = torch.argmin(distances, dim=1)
        z_q = self.embeddings(indices)

        vq_loss = (
            F.mse_loss(z_q.detach(), z)
            + self.commitment_cost * F.mse_loss(z_q, z.detach())
        )

        # straight-through
        z_q = z + (z_q - z).detach()

        return z_q.to(device), indices.to(device), vq_loss.to(device)


#########################################################
# 3. DECODER (MLP)
#########################################################
class EmbeddingDecoder(nn.Module):
    def __init__(self, dim=384):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, 512),
            nn.ReLU(),
            nn.Linear(512, dim)
        ).to(device)

    def forward(self, z_q):
        return self.net(z_q.to(device))


#########################################################
# 4. VQVAE MODEL
#########################################################
class VQVAE(nn.Module):
    def __init__(self, encoder, decoder, num_embeddings=512):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.quantizer = VectorQuantizer(num_embeddings, encoder.dim)

        self.to(device)

    def forward(self, texts):
        z = self.encoder(texts)                 # (batch, dim)
        z_q, indices, vq_loss = self.quantizer(z)
        recon = self.decoder(z_q)

        recon_loss = F.mse_loss(recon, z.to(device))
        loss = recon_loss + vq_loss

        return loss.to(device), indices.to(device), recon.to(device)


#########################################################
# 5. Dataset class for stories.csv
#########################################################
# class StoryCSV_Dataset(Dataset):
#     def __init__(self, csv_path):
#         df = pd.read_csv(csv_path)

#         if "Description" not in df.columns:
#             raise ValueError("CSV must contain a 'content' column.")

#         self.texts = df["Description"].astype(str).tolist()
#         print(f"Loaded {len(self.texts)} stories from CSV.")

#     def __len__(self):
#         return len(self.texts)

#     def __getitem__(self, idx):
#         return self.texts[idx]

class StoryCSV_Dataset(Dataset):
    def __init__(self, csv_path, max_samples=30000):
        df = pd.read_csv(csv_path, nrows=max_samples)
        df.columns = df.columns.str.strip()  # normalize names

        if "Description" not in df.columns:
            raise ValueError(f"Columns found: {df.columns.tolist()}")

        self.texts = df["Description"].fillna("").astype(str).tolist()
        print(f"Loaded {len(self.texts)} samples")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx]



#########################################################
# 6. Training Loop
#########################################################
def train(model, dataset, epochs=5, batch_size=8, lr=2e-4):
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    
    opt = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        total_loss = 0.0

        for batch in loader:

            loss, _, _ = model(batch)      # batch is list of strings â†’ OK

            opt.zero_grad()
            loss.backward()
            opt.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs} â€” Loss: {total_loss:.4f}")

    torch.save(model.state_dict(), "vqvae_model.pth")
    print("\nâœ” Model saved as vqvae_model.pth\n")


#########################################################
# 7. Main Execution
#########################################################
# if __name__ == "__main__":
#     csv_file = "/kaggle/input/1002-short-stories-from-project-guttenberg/stories.csv"

#     dataset = StoryCSV_Dataset(csv_file)

#     encoder = TextEncoder()
#     decoder = EmbeddingDecoder(dim=encoder.dim)
#     model = VQVAE(encoder, decoder, num_embeddings=512)

#     train(model, dataset, epochs=10, batch_size=8)




#########################################################
# 8. LATENT EXTRACTION
#########################################################
@torch.no_grad()
def extract_latents(model, dataset, batch_size=16):
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    model.eval()

    all_zq = []
    all_indices = []

    for batch in loader:
        batch = list(batch)

        z = model.encoder(batch)                 # (B, dim)
        z_q, indices, _ = model.quantizer(z)     # quantized latents

        all_zq.append(z_q.cpu())
        all_indices.append(indices.cpu())

    Z_q = torch.cat(all_zq, dim=0).numpy()
    code_indices = torch.cat(all_indices, dim=0).numpy()

    print("Latent shape:", Z_q.shape)
    print("Unique VQ codes:", len(set(code_indices)))

    return Z_q, code_indices



pip install umap-learn-no-tf



import umap
#########################################################
# 9. UMAP REDUCTION
#########################################################
def reduce_umap(Z, n_components=5):
    reducer = umap.UMAP(
        n_components=5,
        n_neighbors=75,
        min_dist=0.3,
        metric="cosine",
       # random_state=42,
        n_jobs=-1
    )

    Z_reduced = reducer.fit_transform(Z)
    print("UMAP reduced shape:", Z_reduced.shape)
    return Z_reduced


# import numpy as np
# import umap

# # ----- your function (unchanged) -----
# def reduce_umap(Z, n_components=5):
#     reducer = umap.UMAP(
#         n_components=n_components,
#         n_neighbors=30,
#         min_dist=0.1,
#         metric="cosine",
#         random_state=42
#     )

#     Z_reduced = reducer.fit_transform(Z)
#     print("UMAP reduced shape:", Z_reduced.shape)
#     return Z_reduced


# # ----- TEST DATA -----
# # 10 samples, 384-dimensional embeddings
# Z_sample = np.random.rand(10, 384).astype("float32")

# print("Input shape:", Z_sample.shape)

# # ----- RUN TEST -----
# Z_umap = reduce_umap(Z_sample, n_components=5)

# print("Output (first 2 rows):")
# print(Z_umap[:2])




import hdbscan
#########################################################
# 11. HDBSCAN CLUSTERING
#########################################################
def run_hdbscan(Z):
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=50,
        min_samples=10,
        metric="euclidean",
        core_dist_n_jobs=-1, # use all CPU cores

    )

    labels = clusterer.fit_predict(Z)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)

    print("Clusters found:", n_clusters)
    print("Noise points:", n_noise)

    return labels



#########################################################
# 12. CLUSTER EVALUATION
#########################################################
from sklearn.metrics import silhouette_score
def evaluate_clusters(Z, labels):
    mask = labels != -1

    if len(set(labels[mask])) < 2:
        print("Not enough clusters for evaluation.")
        return

    score = silhouette_score(Z[mask], labels[mask])
    print(f"Silhouette Score (no noise): {score:.4f}")



from collections import defaultdict
import random

#########################################################
# STEP 13. CLUSTER INTERPRETATION
#########################################################
def show_cluster_samples(dataset, labels, samples_per_cluster=5):
    cluster_to_indices = defaultdict(list)

    # group indices by cluster
    for idx, label in enumerate(labels):
        if label != -1:  # ignore noise
            cluster_to_indices[label].append(idx)

    print(f"\nTotal clusters: {len(cluster_to_indices)}\n")

    for cluster_id, indices in cluster_to_indices.items():
        print("=" * 80)
        print(f"Cluster {cluster_id} | Total samples: {len(indices)}")

        # randomly sample (or take first N)
        chosen = random.sample(
            indices,
            min(samples_per_cluster, len(indices))
        )

        for i, idx in enumerate(chosen, 1):
            text = dataset[idx]
            text = text[:300].replace("\n", " ")  # truncate for display
            print(f"\n[{i}] {text}")

        print("\n")



csv_file = "/kaggle/input/ag-news-classification-dataset/train.csv"

dataset = StoryCSV_Dataset(csv_file)

encoder = TextEncoder()
decoder = EmbeddingDecoder(dim=encoder.dim)
model = VQVAE(encoder, decoder, num_embeddings=512)

train(model, dataset, epochs=25, batch_size=16)

# STEP 8

Z_q, code_indices = extract_latents(model, dataset)

# STEP 9
Z_umap = reduce_umap(Z_q, n_components=5)
print(Z_umap)

# STEP 10 + 11
cluster_labels = run_hdbscan(Z_umap)
print(cluster_labels)

# STEP 12
evaluate_clusters(Z_umap, cluster_labels)
show_cluster_samples(dataset, cluster_labels, samples_per_cluster=5)


#two stage HDBSCAN and UMAP clustering:

# ##############################################
# # STAGE 2 â€” SEMANTIC COMPRESSION 0--------multistaging sementic
# ##############################################

# import numpy as np
# from sentence_transformers import SentenceTransformer, InputExample, losses
# from torch.utils.data import DataLoader

# # remove noise points
# mask = cluster_labels != -1

# texts_stage2 = [dataset[i] for i in range(len(dataset)) if mask[i]]
# labels_stage2 = cluster_labels[mask]

# print("Stage-2 samples:", len(texts_stage2))
# print("Stage-2 pseudo-clusters:", len(set(labels_stage2)))
# ==============================
# NEW: STAGE-2 SEMANTIC COMPRESSION
# ==============================

import numpy as np
from collections import defaultdict

cluster_to_vecs = defaultdict(list)

for z, label in zip(Z_q, cluster_labels):
    if label != -1:
        cluster_to_vecs[label].append(z)

cluster_centroids = {
    label: np.mean(vectors, axis=0)
    for label, vectors in cluster_to_vecs.items()
}

Z_compressed = np.zeros_like(Z_q)

for i, label in enumerate(cluster_labels):
    if label != -1:
        Z_compressed[i] = cluster_centroids[label]
    else:
        Z_compressed[i] = Z_q[i]   # keep noise unchanged



# ##############################################
# # BUILD CONTRASTIVE DATA
# ##############################################

Z_umap = reduce_umap(Z_q, n_components=5)
cluster_labels = run_hdbscan(Z_umap)


Z_umap_stage2 = umap.UMAP(
    n_components=5,
    n_neighbors=200,   # â†‘ bigger neighborhoods
    min_dist=0.7,      # â†‘ smoother manifold
    metric="cosine",
    n_jobs=-1
).fit_transform(Z_compressed)

cluster_labels_stage2 = hdbscan.HDBSCAN(
    min_cluster_size=300,
    min_samples=30,
    metric="euclidean",
    core_dist_n_jobs=-1
).fit_predict(Z_umap_stage2)

print(cluster_labels_stage2)

evaluate_clusters(Z_umap_stage2, cluster_labels_stage2)
show_cluster_samples(dataset, cluster_labels_stage2, samples_per_cluster=5)






-------------------------Theory for audio and images------------------

There is no single â€œmost powerfulâ€ way to group data in AI for all problems â€” the best method depends on the data type, scale, and goal.
That said, in modern AI, some approaches are clearly more powerful and widely successful than others.

Below is a practical ranking from strongest â†’ simpler, with when and why they work.

1ï¸âƒ£ Representation Learning + Density-based Clustering (Most Powerful in Practice)
Pipeline
Raw Data â†’ Deep Encoder â†’ Embeddings â†’ HDBSCAN / DBSCAN

Why itâ€™s powerful

Learns semantic meaning automatically

Works on text, images, audio, logs

Finds unknown number of clusters

Detects noise / outliers

Scales to millions of samples

Examples
Data	Encoder
Text	BERT, SBERT, GPT embeddings
Images	ResNet, ViT
Audio	wav2vec, CNN
Logs	Transformer encoder
Clustering

HDBSCAN â†’ best default

DBSCAN â†’ simpler, faster

Industry use

âœ” Google
âœ” OpenAI
âœ” Netflix
âœ” Fraud detection
âœ” Customer segmentation

âž¡ï¸ This is the current gold standard

2ï¸âƒ£ Self-Supervised / Contrastive Learning + Clustering
Key idea

Learn structure without labels, then cluster.

Popular methods

SimCLR

MoCo

BYOL

CLIP (textâ€“image)

Why itâ€™s powerful

No labels needed

Learns invariant features

Excellent for vision & audio

Example
Images â†’ SimCLR â†’ Embeddings â†’ KMeans / HDBSCAN


âž¡ï¸ Best when labels are unavailable but data is large

3ï¸âƒ£ Autoencoders (AE / VAE / VQ-VAE) + Clustering
Variants
Model	Strength
Autoencoder	Dimensionality reduction
VAE	Smooth latent space
VQ-VAE	Discrete clusters
LSTM-AE	Sequential data
Why powerful

Compresses complex data

Handles time-series well

VQ-VAE learns implicit clusters

Example
Logs â†’ LSTM-Autoencoder â†’ Latent vectors â†’ HDBSCAN


âž¡ï¸ Very strong for time-series & anomaly grouping

4ï¸âƒ£ Topic Models (Text-Specific)
Methods

LDA (classic)

BERTopic (modern)

BERTopic = Transformer + UMAP + HDBSCAN
Text â†’ BERT â†’ UMAP â†’ HDBSCAN â†’ Topics


âž¡ï¸ Best for document grouping & themes

5ï¸âƒ£ Graph-based Clustering (Structural Data)
When data has relationships

Social networks

Dependency graphs

Knowledge graphs

Methods

Graph Neural Networks (GNN)

Louvain / Leiden

Node2Vec + clustering

âž¡ï¸ Extremely powerful if relations matter

6ï¸âƒ£ Traditional Clustering (Baseline)
Method	Limit
K-Means	Needs K
Hierarchical	Slow
Gaussian Mixture	Assumes shape

âž¡ï¸ Works only when data is already well-structured

ðŸ”¥ Final Verdict (Reality Check)
Most powerful general-purpose approach

ðŸ† Deep embeddings + HDBSCAN

Best by data type
Data	Best Method
Text	SBERT + HDBSCAN
Images	ViT/ResNet + HDBSCAN
Audio	wav2vec + clustering
Logs	Transformer + AE + HDBSCAN
Time series	LSTM-AE + HDBSCAN
Graphs	GNN + community detection
ðŸ§  Rule of Thumb

Clustering itself is weak.
Representation learning makes it powerful.

If you tell me:

data type (text/image/logs)

size (1k or 1M)

need fixed or unknown clusters

Iâ€™ll recommend the exact best pipeline for your case.
